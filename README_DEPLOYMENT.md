# ğŸš€ HÆ°á»›ng Dáº«n Deployment - Chatbot 207 Thá»§ Tá»¥c HÃ nh ChÃ­nh

## ğŸ“‹ Tá»•ng Quan

Há»‡ thá»‘ng bao gá»“m:
- **Backend**: FastAPI (Python) - Port 8000
- **Frontend**: Next.js (React) - Port 3000
- **Vector DB**: Qdrant (local storage)
- **LLM**: Ollama (qwen3:8b, bge-m3)

---

## âœ… Prerequisites

### 1. Ollama

Äáº£m báº£o Ollama Ä‘ang cháº¡y vá»›i cÃ¡c models cáº§n thiáº¿t:

```bash
# Kiá»ƒm tra Ollama Ä‘ang cháº¡y
ollama list

# Cáº§n cÃ³ 2 models:
# - bge-m3 (embedding model)
# - qwen3:8b (LLM model)

# Náº¿u chÆ°a cÃ³, táº£i vá»:
ollama pull bge-m3
ollama pull qwen3:8b
```

### 2. Conda Environment

```bash
# Activate environment
conda activate thu_tuc_rag

# Kiá»ƒm tra Python version (cáº§n >= 3.10)
python --version
```

### 3. Node.js & npm

```bash
# Kiá»ƒm tra Node.js (cáº§n >= 18.0)
node --version

# Kiá»ƒm tra npm
npm --version
```

---

## ğŸ”§ Setup & Installation

### Backend Setup

```bash
# 1. Navigate to project directory
cd /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag

# 2. Activate conda environment
conda activate thu_tuc_rag

# 3. Install backend dependencies (Ä‘Ã£ install rá»“i)
# pip install -r requirements_backend.txt

# 4. Verify .env file exists
cat .env
```

### Frontend Setup

```bash
# 1. Navigate to frontend directory
cd /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag/fe

# 2. Install dependencies (náº¿u chÆ°a install)
npm install

# 3. Verify .env.local file exists
cat .env.local
```

---

## ğŸš€ Starting the Application

### Terminal 1: Start Backend (FastAPI)

```bash
# Activate conda environment
conda activate thu_tuc_rag

# Navigate to project root
cd /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag

# Start backend server
uvicorn backend.main:app --host 0.0.0.0 --port 8000 --reload
```

**Expected output:**
```
============================================================
Starting Thu Tuc RAG API v1.0.0
CORS Origins: ['http://localhost:3000']
Ollama URL: http://localhost:11434
Qdrant Path: /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag/qdrant_storage
============================================================
Initializing RAG pipeline...
RAG pipeline initialized successfully
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     Started reloader process
INFO:     Started server process
INFO:     Waiting for application startup.
INFO:     Application startup complete.
```

**Backend URLs:**
- API Root: http://localhost:8000
- API Docs (Swagger): http://localhost:8000/docs
- Health Check: http://localhost:8000/api/health

### Terminal 2: Start Frontend (Next.js)

```bash
# Navigate to frontend directory
cd /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag/fe

# Start development server
npm run dev
```

**Expected output:**
```
â–² Next.js 16.1.1
- Local:        http://localhost:3000
- Network:      http://0.0.0.0:3000

âœ“ Starting...
âœ“ Ready in 2.3s
```

**Frontend URL:**
- Application: http://localhost:3000

---

## ğŸ§ª Testing the Application

### 1. Test Backend Health

```bash
# In a new terminal
curl http://localhost:8000/api/health | python3 -m json.tool
```

**Expected response:**
```json
{
  "status": "healthy",
  "qdrant_status": "connected",
  "ollama_status": "connected",
  "version": "1.0.0",
  "timestamp": "2025-12-29T..."
}
```

### 2. Test Backend API

```bash
curl -X POST http://localhost:8000/api/chat/query \
  -H "Content-Type: application/json" \
  -d '{"query": "ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬?"}' \
  | python3 -m json.tool
```

### 3. Test Frontend

1. Má»Ÿ browser: http://localhost:3000
2. Nháº­p cÃ¢u há»i: "ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬?"
3. Äá»£i pháº£n há»“i (60-180 giÃ¢y do Ollama local)
4. Kiá»ƒm tra:
   - âœ… CÃ¢u tráº£ lá»i hiá»ƒn thá»‹
   - âœ… Source citations cÃ³ thá»ƒ má»Ÿ/Ä‘Ã³ng
   - âœ… Structured data (JSON) cÃ³ thá»ƒ xem
   - âœ… Chat history Ä‘Æ°á»£c lÆ°u khi refresh page

---

## ğŸ“‚ File Structure

```
thu_tuc_rag/
â”œâ”€â”€ backend/                      # FastAPI backend
â”‚   â”œâ”€â”€ main.py                   # Entry point
â”‚   â”œâ”€â”€ config.py                 # Configuration
â”‚   â”œâ”€â”€ dependencies.py           # DI container
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â”œâ”€â”€ chat.py           # Chat endpoints
â”‚   â”‚   â”‚   â””â”€â”€ health.py         # Health check
â”‚   â”‚   â””â”€â”€ models/
â”‚   â”‚       â”œâ”€â”€ request.py        # Request schemas
â”‚   â”‚       â””â”€â”€ response.py       # Response schemas
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ chat_service.py       # Core business logic
â”‚   â”‚   â””â”€â”€ session_manager.py   # Session management
â”‚   â””â”€â”€ middleware/
â”‚       â””â”€â”€ error_handler.py      # Error handling
â”‚
â”œâ”€â”€ fe/                           # Next.js frontend
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx              # Main page
â”‚   â”‚   â”œâ”€â”€ layout.tsx            # Root layout
â”‚   â”‚   â””â”€â”€ globals.css           # Global styles
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ui/                   # UI components
â”‚   â”‚   â””â”€â”€ chat/                 # Chat components
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â”œâ”€â”€ useChat.ts            # Chat state management
â”‚   â”‚   â””â”€â”€ useLocalStorage.ts   # LocalStorage hook
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ types.ts              # TypeScript interfaces
â”‚       â”œâ”€â”€ api-client.ts         # API client
â”‚       â””â”€â”€ utils.ts              # Utilities
â”‚
â”œâ”€â”€ src/                          # RAG pipeline (existing)
â”œâ”€â”€ data/                         # Data files (existing)
â”œâ”€â”€ qdrant_storage/               # Vector database (existing)
â”‚
â”œâ”€â”€ .env                          # Backend environment variables
â”œâ”€â”€ requirements_backend.txt      # Backend dependencies
â””â”€â”€ README_DEPLOYMENT.md          # This file
```

---

## ğŸ” Troubleshooting

### Backend Issues

**Port 8000 already in use:**
```bash
# Find and kill process using port 8000
lsof -ti :8000 | xargs kill -9

# Or use different port
uvicorn backend.main:app --port 8001
```

**Ollama connection error:**
```bash
# Check Ollama is running
curl http://localhost:11434/api/tags

# Restart Ollama if needed
systemctl restart ollama  # or restart Ollama app
```

**Qdrant connection error:**
```bash
# Check Qdrant storage path exists
ls -la /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag/qdrant_storage

# Re-index if needed (if you have indexing script)
# python src/retrieval/index_to_qdrant.py
```

**Import errors:**
```bash
# Make sure you're in the right directory
cd /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag

# Make sure conda environment is activated
conda activate thu_tuc_rag

# Reinstall dependencies if needed
pip install -r requirements_backend.txt
```

### Frontend Issues

**Module not found errors:**
```bash
cd /home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag/fe

# Clear cache and reinstall
rm -rf node_modules package-lock.json
npm install
```

**CORS errors:**
```bash
# Verify backend CORS settings in .env
cat ../.env | grep CORS

# Should be: CORS_ORIGINS=["http://localhost:3000"]
```

**API connection errors:**
```bash
# Verify .env.local
cat .env.local

# Should be: NEXT_PUBLIC_API_URL=http://localhost:8000

# Restart frontend after changing .env.local
npm run dev
```

---

## âš™ï¸ Configuration

### Backend (.env)

```bash
# Ollama
OLLAMA_URL=http://localhost:11434
EMBEDDING_MODEL=bge-m3
LLM_MODEL=qwen3:8b

# Qdrant
QDRANT_PATH=/home/admin123/Downloads/NHDanDz/ThuTucHanhChinh/thu_tuc_rag/qdrant_storage

# Session
SESSION_TTL_SECONDS=3600

# CORS
CORS_ORIGINS=["http://localhost:3000"]

# Server
HOST=0.0.0.0
PORT=8000
```

### Frontend (.env.local)

```bash
NEXT_PUBLIC_API_URL=http://localhost:8000
```

---

## ğŸ“Š Performance Expectations

### Response Times (Local Ollama)

- **Query Enhancement**: 2-5 seconds
- **Vector Retrieval**: 0.1-0.5 seconds
- **Answer Generation**: **50-180 seconds** âš ï¸
- **Total**: ~60-190 seconds per query

**Note:** Ollama local inference is slow. For production, consider using cloud LLM (OpenAI GPT-4, Claude 3.5) for 10-50x speedup.

### Concurrent Users

- **Local Ollama**: 1-2 concurrent users
- **Session Storage**: 1000 sessions max (in-memory)

---

## ğŸ”„ Stopping the Application

### Stop Backend

In the terminal running backend:
```bash
# Press Ctrl+C
^C
```

### Stop Frontend

In the terminal running frontend:
```bash
# Press Ctrl+C
^C
```

---

## ğŸ“ Usage Examples

### Basic Query

```
User: "ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬?"