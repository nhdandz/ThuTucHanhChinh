#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Cháº¡y Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§ trÃªn Comprehensive Test Dataset
"""

import sys
import os
from pathlib import Path

# Add parent directories to path
current_dir = Path(__file__).parent
src_dir = current_dir.parent
sys.path.insert(0, str(src_dir))

from evaluation.test_dataset import TestDatasetManager
from evaluation.evaluator import RAGEvaluator
from evaluation.metrics import MetricsCalculator

# OPTION 1: Sá»­ dá»¥ng RAG Pipeline tháº­t (náº¿u Ä‘Ã£ cÃ³)
USE_REAL_RAG = True  # Äá»•i thÃ nh True khi Ä‘Ã£ cÃ³ RAG pipeline VÃ€ dá»¯ liá»‡u trong DB

if USE_REAL_RAG:
    # Import RAG pipeline
    from pipeline.rag_pipeline import ThuTucRAGPipeline

    # Khá»Ÿi táº¡o pipeline
    rag_pipeline = ThuTucRAGPipeline(
        vector_store_path="../retrieval/qdrant_storage",  # ÄÆ°á»ng dáº«n Ä‘áº¿n Qdrant DB
        embedding_model="bge-m3",                         # Model embedding
        llm_model="qwen3:8b",                             # Model LLM (khÃ´ng pháº£i model_name!)
        ollama_url="http://localhost:11434"
    )

    def answer_generator_fn(question: str):
        """HÃ m táº¡o cÃ¢u tráº£ lá»i tá»« RAG pipeline tháº­t"""
        import time

        start_time = time.time()

        # Gá»i RAG pipeline
        result = rag_pipeline.answer_question(question, verbose=False)

        total_time = time.time() - start_time

        # GeneratedAnswer cÃ³ cÃ¡c field: answer, structured_data, sources, etc.
        return {
            'answer': result.answer,  # Natural language answer
            'retrieval_time': total_time * 0.6,  # Estimate: 60% retrieval
            'generation_time': total_time * 0.4,  # Estimate: 40% generation
            'chunks_retrieved': len(result.sources)  # Sá»‘ lÆ°á»£ng source chunks
        }

else:
    # OPTION 2: Sá»­ dá»¥ng mock answer (cho demo)
    print("âš ï¸  ÄANG DÃ™NG MOCK DATA - Äá»•i USE_REAL_RAG = True Ä‘á»ƒ dÃ¹ng RAG tháº­t")
    print()

    def answer_generator_fn(question: str):
        """HÃ m mock - tráº£ vá» cÃ¢u tráº£ lá»i giáº£"""

        # Mock answers cho má»™t sá»‘ cÃ¢u há»i
        mock_answers = {
            "ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬?": """
Há»“ sÆ¡ Ä‘Äƒng kÃ½ káº¿t hÃ´n gá»“m:
1. Giáº¥y tá» tÃ¹y thÃ¢n (CMND/CCCD/Há»™ chiáº¿u) cá»§a cáº£ hai bÃªn - 02 báº£n sao
2. Giáº¥y xÃ¡c nháº­n tÃ¬nh tráº¡ng hÃ´n nhÃ¢n - 01 báº£n chÃ­nh (Ä‘á»‘i vá»›i ngÆ°á»i tá»« 30 tuá»•i trá»Ÿ lÃªn)
3. Giáº¥y khÃ¡m sá»©c khá»e tiá»n hÃ´n nhÃ¢n - 01 báº£n chÃ­nh
4. ÄÆ¡n Ä‘Äƒng kÃ½ káº¿t hÃ´n - 01 báº£n
            """.strip(),

            "ÄÄƒng kÃ½ káº¿t hÃ´n máº¥t bao lÃ¢u?": """
Thá»i gian giáº£i quyáº¿t: Trong ngÃ y lÃ m viá»‡c.
TrÆ°á»ng há»£p Ä‘áº·c biá»‡t: KhÃ´ng quÃ¡ 03 ngÃ y lÃ m viá»‡c.
            """.strip(),

            "Cáº¥p CCCD cáº§n nhá»¯ng giáº¥y tá» nÃ o?": """
Há»“ sÆ¡ cáº¥p CCCD:
1. Giáº¥y khai sinh - 01 báº£n sao
2. Sá»• há»™ kháº©u hoáº·c giáº¥y xÃ¡c nháº­n cÆ° trÃº - 01 báº£n sao
3. áº¢nh 4x6 - 02 áº£nh
            """.strip()
        }

        # Láº¥y mock answer hoáº·c tráº£ vá» cÃ¢u tráº£ lá»i máº·c Ä‘á»‹nh
        answer = mock_answers.get(
            question,
            "ÄÃ¢y lÃ  cÃ¢u tráº£ lá»i mock cho cÃ¢u há»i: " + question
        )

        return {
            'answer': answer,
            'retrieval_time': 5.0,
            'generation_time': 30.0,
            'chunks_retrieved': 3
        }


def main():
    """Cháº¡y Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§"""

    print("=" * 80)
    print("ÄÃNH GIÃ TOÃ€N DIá»†N Há»† THá»NG RAG")
    print("=" * 80)
    print()

    # 1. Load comprehensive test dataset
    print("ğŸ“‚ Äang load test dataset...")
    manager = TestDatasetManager()
    dataset_path = current_dir / "comprehensive_test_dataset.json"
    manager.load_dataset(str(dataset_path))

    print(f"âœ… ÄÃ£ load {len(manager.test_cases)} test cases")
    print()

    # Hiá»ƒn thá»‹ thá»‘ng kÃª
    stats = manager.get_statistics()
    print("ğŸ“Š Thá»‘ng kÃª dataset:")
    print(f"   Tá»•ng sá»‘: {stats['total_cases']} cases")
    print(f"   Theo category:")
    for cat, count in stats['by_category'].items():
        print(f"      {cat}: {count} cases")
    print(f"   Theo difficulty:")
    for diff, count in stats['by_difficulty'].items():
        print(f"      {diff}: {count} cases")
    print()

    # 2. Khá»Ÿi táº¡o evaluator
    print("ğŸ”§ Khá»Ÿi táº¡o evaluator...")

    # Táº¡o MetricsCalculator vá»›i thresholds
    metrics_calculator = MetricsCalculator(
        accuracy_threshold=0.95,
        precision_threshold=0.90,
        recall_threshold=0.90,
        f1_threshold=0.90,
        hallucination_threshold=0.05
    )

    # Táº¡o RAGEvaluator vá»›i MetricsCalculator
    evaluator = RAGEvaluator(metrics_calculator=metrics_calculator)
    print()

    # 3. Chá»n subset Ä‘á»ƒ test (báº¯t Ä‘áº§u vá»›i Ã­t cases)
    print("ğŸ¯ Chá»n test cases Ä‘á»ƒ Ä‘Ã¡nh giÃ¡:")
    print("   1. Test 5 cases Ä‘áº§u tiÃªn (nhanh)")
    print("   2. Test theo category")
    print("   3. Test theo difficulty")
    print("   4. Test táº¥t cáº£ 54 cases (Ä‘áº§y Ä‘á»§)")
    print()

    # Máº·c Ä‘á»‹nh: test 5 cases Ä‘áº§u
    test_mode = input("Chá»n mode (1-4, máº·c Ä‘á»‹nh=1): ").strip() or "1"
    print()

    if test_mode == "1":
        test_cases = manager.test_cases[:5]
        print(f"âœ… Äang test 5 cases Ä‘áº§u tiÃªn")
    elif test_mode == "2":
        print("Categories cÃ³ sáºµn:", list(stats['by_category'].keys()))
        category = input("Chá»n category: ").strip()
        test_cases = manager.filter_by_category(category)
        print(f"âœ… Äang test {len(test_cases)} cases trong category '{category}'")
    elif test_mode == "3":
        print("Difficulties: easy, medium, hard")
        difficulty = input("Chá»n difficulty: ").strip()
        test_cases = manager.filter_by_difficulty(difficulty)
        print(f"âœ… Äang test {len(test_cases)} cases vá»›i difficulty '{difficulty}'")
    else:
        test_cases = manager.test_cases
        print(f"âœ… Äang test Táº¤T Cáº¢ {len(test_cases)} cases")

    print()

    # 4. Cháº¡y evaluation
    print("ğŸ§ª Báº¯t Ä‘áº§u Ä‘Ã¡nh giÃ¡...")
    print()

    report = evaluator.evaluate_batch(
        test_cases=test_cases,
        answer_generator_fn=answer_generator_fn,
        verbose=True
    )

    # 5. Hiá»ƒn thá»‹ káº¿t quáº£
    print()
    print(evaluator.format_evaluation_report(report))

    # 6. LÆ°u report (optional)
    save_report = input("\nLÆ°u report vÃ o file? (y/n, máº·c Ä‘á»‹nh=n): ").strip().lower()
    if save_report == 'y':
        report_file = f"evaluation_report_{report.report_id}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(evaluator.format_evaluation_report(report))
        print(f"âœ… ÄÃ£ lÆ°u report: {report_file}")

    print()
    print("=" * 80)
    print("HOÃ€N THÃ€NH ÄÃNH GIÃ")
    print("=" * 80)

    # TÃ³m táº¯t káº¿t quáº£
    if report.overall_accuracy >= 0.95:
        print("ğŸ‰ Há»† THá»NG Äáº T Má»¨C PRODUCTION-READY!")
    elif report.overall_accuracy >= 0.80:
        print("âš ï¸  Há»‡ thá»‘ng cáº§n cáº£i thiá»‡n thÃªm")
    else:
        print("âŒ Há»‡ thá»‘ng cáº§n Ä‘iá»u chá»‰nh nhiá»u")

    print(f"\nğŸ“Š Káº¿t quáº£ tá»•ng quÃ¡t:")
    print(f"   Accuracy: {report.overall_accuracy:.2%} (Target: â‰¥95%)")
    print(f"   Pass Rate: {report.pass_rate:.2%}")
    print(f"   Hallucination: {report.average_hallucination_rate:.2%} (Target: â‰¤5%)")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ÄÃ¡nh giÃ¡ bá»‹ há»§y bá»Ÿi user")
    except Exception as e:
        print(f"\n\nâŒ Lá»—i: {e}")
        import traceback
        traceback.print_exc()
