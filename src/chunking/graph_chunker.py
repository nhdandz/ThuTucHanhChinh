#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Enhanced Graph-Based Hierarchical Chunking Strategy for Administrative Procedures
Extends HierarchicalChunker with:
- Enriched chunk fields (parent_context, breadcrumb, importance_score)
- Graph relationships (sibling_chunk_ids, related_procedure_ids)
- Two new chunk types: child_fees_timing, child_agencies
- Complete information preservation from source documents

CRITICAL: All 20 fields from source .doc files must be preserved in chunks
"""

import sys
import json
from pathlib import Path
from typing import Dict, List, Optional
import tiktoken
from dataclasses import dataclass, asdict, field

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')


@dataclass
class EnrichedChunk:
    """
    Enhanced Chunk data structure with graph relationships and enriched context

    IMPORTANT: All fields from original Chunk class preserved for backward compatibility
    New fields added for graph-based enrichment and improved retrieval
    """
    # ===== CORE FIELDS (REQUIRED) =====
    chunk_id: str
    thu_tuc_id: str
    chunk_type: str  # parent_overview, child_documents, child_requirements, child_process, child_legal, child_fees_timing, child_agencies
    chunk_tier: str  # parent or child
    parent_chunk_id: Optional[str]
    content: str
    metadata: Dict
    char_count: int
    token_count: int

    # ===== NEW ENRICHED FIELDS (WITH DEFAULTS) =====
    # Graph relationships for sibling enrichment
    sibling_chunk_ids: List[str] = field(default_factory=list)  # Other chunks from same procedure
    related_procedure_ids: List[str] = field(default_factory=list)  # Related procedures (same lƒ©nh v·ª±c, similar)

    # Enriched context for better embeddings
    parent_context: str = ""  # First 200 chars of parent chunk for context
    breadcrumb: str = ""  # "Lƒ©nh v·ª±c > Procedure name > Section"

    # Metadata for scoring and filtering
    importance_score: float = 0.5  # 0-1, based on chunk type and content
    complexity_level: str = "medium"  # "simple", "medium", "complex"


class GraphChunker:
    """
    Enhanced Hierarchical + Graph-Based Chunker

    Creates:
    - 1 Parent chunk (overview of all 20 fields)
    - 6 Child chunk types:
      1. child_documents (Th√†nh ph·∫ßn h·ªì s∆°)
      2. child_requirements (Y√™u c·∫ßu & ƒêi·ªÅu ki·ªán)
      3. child_process (Tr√¨nh t·ª± & C√°ch th·ª©c th·ª±c hi·ªán)
      4. child_legal (CƒÉn c·ª© ph√°p l√Ω)
      5. child_fees_timing (NEW: Ph√≠ l·ªá ph√≠ + Th·ªùi h·∫°n + H√¨nh th·ª©c n·ªôp - ALL items)
      6. child_agencies (NEW: T·∫•t c·∫£ c∆° quan + ƒë·ªãa ch·ªâ)

    Enriches all chunks with:
    - Parent context (first 200 chars)
    - Breadcrumb path (Lƒ©nh v·ª±c > Procedure > Section)
    - Sibling chunk IDs
    - Importance score
    """

    # Chunk parameters (existing + new)
    CHUNK_PARAMS = {
        "parent_overview": {
            "max_tokens": 512,
            "overlap": 0,
            "priority": "always_retrieve"
        },
        "child_documents": {
            "max_tokens": 1024,
            "overlap": 100,
            "preserve_structure": True,
            "separators": ["\n\n", "\nSTT", "\n1.", "\n2."]
        },
        "child_requirements": {
            "max_tokens": 768,
            "overlap": 200,
            "separators": ["\n\nƒêi·ªÅu ki·ªán", "\n\nY√™u c·∫ßu", "\n-", ". ", "; "]
        },
        "child_process": {
            "max_tokens": 896,
            "overlap": 150,
            "separators": ["\n\nB∆∞·ªõc", "\nB∆∞·ªõc", ". "]
        },
        "child_legal": {
            "max_tokens": 512,
            "overlap": 50,
            "separators": ["\n\n", "\n1.", "\n2."]
        },
        # NEW CHUNK TYPES
        "child_fees_timing": {
            "max_tokens": 512,
            "overlap": 0,
            "preserve_structure": True
        },
        "child_agencies": {
            "max_tokens": 640,
            "overlap": 0,
            "preserve_structure": True
        }
    }

    # Importance scoring by chunk type
    CHUNK_IMPORTANCE = {
        "parent_overview": 1.0,  # Highest - always needed
        "child_documents": 0.9,  # Very high - required docs
        "child_requirements": 0.85,  # High - eligibility
        "child_process": 0.8,  # High - how to do it
        "child_fees_timing": 0.75,  # Medium-high - cost & time
        "child_agencies": 0.7,  # Medium - where to go
        "child_legal": 0.6  # Medium - legal references
    }

    def __init__(self, encoding_name: str = "cl100k_base", procedure_graph=None):
        """
        Initialize with tiktoken encoder

        Args:
            encoding_name: Tokenizer encoding name
            procedure_graph: Optional ProcedureGraph for relationship enrichment
        """
        self.encoder = tiktoken.get_encoding(encoding_name)
        self.chunks = []
        self.procedure_graph = procedure_graph

    def count_tokens(self, text: str) -> int:
        """Count tokens in text"""
        return len(self.encoder.encode(text))

    def chunk_thu_tuc(self, thu_tuc_data: Dict) -> List[EnrichedChunk]:
        """
        Main function: Chunk m·ªôt th·ªß t·ª•c th√†nh parent + enriched child chunks

        IMPORTANT: Preserves ALL information from source data
        """
        thu_tuc_id = thu_tuc_data["thu_tuc_id"]
        chunks = []

        # TIER 1: Create Parent chunk (overview of ALL 20 fields)
        parent_chunk = self._create_parent_chunk(thu_tuc_data)
        chunks.append(parent_chunk)

        # TIER 2: Create Child chunks (existing + new types)
        # Child A: Documents (Th√†nh ph·∫ßn h·ªì s∆°)
        docs_chunks = self._create_documents_chunks(thu_tuc_data, parent_chunk.chunk_id)
        chunks.extend(docs_chunks)

        # Child B: Requirements (Y√™u c·∫ßu & ƒêi·ªÅu ki·ªán)
        req_chunks = self._create_requirements_chunks(thu_tuc_data, parent_chunk.chunk_id)
        chunks.extend(req_chunks)

        # Child C: Process (Quy tr√¨nh)
        process_chunks = self._create_process_chunks(thu_tuc_data, parent_chunk.chunk_id)
        chunks.extend(process_chunks)

        # Child D: Legal Basis (CƒÉn c·ª© ph√°p l√Ω)
        legal_chunks = self._create_legal_chunks(thu_tuc_data, parent_chunk.chunk_id)
        chunks.extend(legal_chunks)

        # NEW: Child E: Fees & Timing (Ph√≠ l·ªá ph√≠ + Th·ªùi h·∫°n + H√¨nh th·ª©c n·ªôp)
        fees_chunks = self._create_fees_timing_chunks(thu_tuc_data, parent_chunk.chunk_id)
        chunks.extend(fees_chunks)

        # NEW: Child F: Agencies (T·∫•t c·∫£ c∆° quan + ƒë·ªãa ch·ªâ)
        agency_chunks = self._create_agencies_chunks(thu_tuc_data, parent_chunk.chunk_id)
        chunks.extend(agency_chunks)

        # ENRICHMENT: Add parent context, breadcrumbs, sibling IDs, importance scores
        enriched_chunks = self._enrich_all_chunks(chunks, parent_chunk, thu_tuc_data)

        return enriched_chunks

    def _create_parent_chunk(self, data: Dict) -> EnrichedChunk:
        """
        TIER 1: Parent Chunk - Master Overview of ALL 20 fields

        IMPORTANT: This chunk summarizes ALL information from the procedure
        """
        meta = data["metadata"]
        content_data = data["content"]
        tables = data["tables"]

        # Extract timing & fees from table
        thoi_han = ""
        phi_le_phi = ""
        if tables.get("hinh_thuc_nop") and len(tables["hinh_thuc_nop"]) > 0:
            first_form = tables["hinh_thuc_nop"][0]
            thoi_han = first_form.get("thoi_han_giai_quyet", "")
            phi_le_phi = first_form.get("phi_le_phi", "")

        # Build comprehensive content covering all 20 fields
        content = f"""TH·ª¶ T·ª§C: {meta.get('t√™n_th·ªß_t·ª•c', '')}
M√É: {meta.get('m√£_th·ªß_t·ª•c', '')}
Lƒ®NH V·ª∞C: {meta.get('lƒ©nh_v·ª±c', '')}
LO·∫†I: {meta.get('lo·∫°i_th·ªß_t·ª•c', '')}
C·∫§P TH·ª∞C HI·ªÜN: {meta.get('c·∫•p_th·ª±c_hi·ªán', '')}
S·ªê QUY·∫æT ƒê·ªäNH: {meta.get('s·ªë_quy·∫øt_ƒë·ªãnh', '')}

T√ìM T·∫ÆT:
- ƒê·ªëi t∆∞·ª£ng: {content_data.get('ƒë·ªëi_t∆∞·ª£ng_th·ª±c_hi·ªán', 'Kh√¥ng c√≥ th√¥ng tin')}
- C∆° quan th·ª±c hi·ªán: {content_data.get('c∆°_quan_th·ª±c_hi·ªán', 'Kh√¥ng c√≥ th√¥ng tin')}
- C∆° quan th·∫©m quy·ªÅn: {content_data.get('c∆°_quan_c√≥_th·∫©m_quy·ªÅn', 'Kh√¥ng c√≥ th√¥ng tin')}
- K·∫øt qu·∫£: {content_data.get('k·∫øt_qu·∫£_th·ª±c_hi·ªán', 'Kh√¥ng c√≥ th√¥ng tin')}
- Th·ªùi gian: {thoi_han}
- Chi ph√≠: {phi_le_phi}

‚Üí Chi ti·∫øt v·ªÅ: Gi·∫•y t·ªù c·∫ßn n·ªôp, Y√™u c·∫ßu ƒëi·ªÅu ki·ªán, Quy tr√¨nh th·ª±c hi·ªán, Ph√≠ & th·ªùi gian, C∆° quan, CƒÉn c·ª© ph√°p l√Ω (xem chunks con)
"""

        chunk_id = f"{data['thu_tuc_id']}_parent_overview"

        chunk = EnrichedChunk(
            chunk_id=chunk_id,
            thu_tuc_id=data["thu_tuc_id"],
            chunk_type="parent_overview",
            chunk_tier="parent",
            parent_chunk_id=None,
            content=content.strip(),
            metadata={
                "m√£_th·ªß_t·ª•c": meta.get('m√£_th·ªß_t·ª•c', ''),
                "t√™n_th·ªß_t·ª•c": meta.get('t√™n_th·ªß_t·ª•c', ''),
                "lƒ©nh_v·ª±c": meta.get('lƒ©nh_v·ª±c', ''),
                "lo·∫°i_th·ªß_t·ª•c": meta.get('lo·∫°i_th·ªß_t·ª•c', ''),
                "c·∫•p_th·ª±c_hi·ªán": meta.get('c·∫•p_th·ª±c_hi·ªán', ''),
                "s·ªë_quy·∫øt_ƒë·ªãnh": meta.get('s·ªë_quy·∫øt_ƒë·ªãnh', '')
            },
            char_count=len(content),
            token_count=self.count_tokens(content),
            importance_score=self.CHUNK_IMPORTANCE["parent_overview"]
        )

        return chunk

    # ========== EXISTING CHUNK METHODS (PRESERVED) ==========
    # Keep all existing logic intact for backward compatibility

    def _create_documents_chunks(self, data: Dict, parent_id: str) -> List[EnrichedChunk]:
        """
        Child Type A: Documents (Th√†nh ph·∫ßn h·ªì s∆°)

        PRESERVED: Original logic maintained
        """
        chunks = []
        thanh_phan_ho_so = data["tables"].get("thanh_phan_ho_so", [])

        if not thanh_phan_ho_so or len(thanh_phan_ho_so) == 0:
            return chunks

        # Build parent context
        parent_context = f"""[PARENT CONTEXT]
Th·ªß t·ª•c: {data['metadata'].get('t√™n_th·ªß_t·ª•c', '')}
M√£: {data['metadata'].get('m√£_th·ªß_t·ª•c', '')}
"""

        # Build documents list
        main_content = "[MAIN CONTENT]\nTH√ÄNH PH·∫¶N H·ªí S∆† C·∫¶N N·ªòP:\n\n"

        for i, doc in enumerate(thanh_phan_ho_so, 1):
            ten_giay_to = doc.get("ten_giay_to", "")
            so_luong = doc.get("so_luong", "")
            ghi_chu = doc.get("ghi_chu", "")

            doc_text = f"{i}. {ten_giay_to}\n"
            if so_luong:
                doc_text += f"   - S·ªë l∆∞·ª£ng: {so_luong}\n"
            if ghi_chu:
                doc_text += f"   - Ghi ch√∫: {ghi_chu}\n"
            doc_text += "\n"

            main_content += doc_text

        # Build submission methods
        hinh_thuc_section = "\nC√ÅCH TH·ª®C N·ªòP H·ªí S∆†:\n"
        hinh_thuc_nop = data["tables"].get("hinh_thuc_nop", [])
        if hinh_thuc_nop:
            for form in hinh_thuc_nop:
                hinh_thuc = form.get("hinh_thuc", "")
                thoi_han = form.get("thoi_han_giai_quyet", "")
                phi = form.get("phi_le_phi", "")
                hinh_thuc_section += f"- {hinh_thuc}: Th·ªùi h·∫°n {thoi_han}, {phi}\n"

        dia_chi = data["content"].get("ƒë·ªãa_ch·ªâ_ti·∫øp_nh·∫≠n_hs", "")
        if dia_chi and dia_chi != "Kh√¥ng c√≥ th√¥ng tin":
            hinh_thuc_section += f"\nƒê·ªãa ch·ªâ ti·∫øp nh·∫≠n: {dia_chi}\n"

        main_content += hinh_thuc_section

        # Check token count
        full_content = parent_context + "\n" + main_content
        token_count = self.count_tokens(full_content)

        # If < 1024 tokens: single chunk
        if token_count <= self.CHUNK_PARAMS["child_documents"]["max_tokens"]:
            chunk = EnrichedChunk(
                chunk_id=f"{data['thu_tuc_id']}_child_documents_0",
                thu_tuc_id=data["thu_tuc_id"],
                chunk_type="child_documents",
                chunk_tier="child",
                parent_chunk_id=parent_id,
                content=full_content.strip(),
                metadata=data["metadata"].copy(),
                char_count=len(full_content),
                token_count=token_count,
                importance_score=self.CHUNK_IMPORTANCE["child_documents"]
            )
            chunks.append(chunk)
        else:
            # Split into groups of 5 documents
            chunk_size = 5
            for i in range(0, len(thanh_phan_ho_so), chunk_size):
                group = thanh_phan_ho_so[i:i+chunk_size]

                group_content = "[MAIN CONTENT]\nTH√ÄNH PH·∫¶N H·ªí S∆† C·∫¶N N·ªòP:\n\n"
                for j, doc in enumerate(group, i+1):
                    ten_giay_to = doc.get("ten_giay_to", "")
                    so_luong = doc.get("so_luong", "")
                    ghi_chu = doc.get("ghi_chu", "")

                    doc_text = f"{j}. {ten_giay_to}\n"
                    if so_luong:
                        doc_text += f"   - S·ªë l∆∞·ª£ng: {so_luong}\n"
                    if ghi_chu:
                        doc_text += f"   - Ghi ch√∫: {ghi_chu}\n"
                    doc_text += "\n"

                    group_content += doc_text

                # Add submission info to last chunk
                if i + chunk_size >= len(thanh_phan_ho_so):
                    group_content += hinh_thuc_section

                chunk_content = parent_context + "\n" + group_content

                chunk = EnrichedChunk(
                    chunk_id=f"{data['thu_tuc_id']}_child_documents_{i//chunk_size}",
                    thu_tuc_id=data["thu_tuc_id"],
                    chunk_type="child_documents",
                    chunk_tier="child",
                    parent_chunk_id=parent_id,
                    content=chunk_content.strip(),
                    metadata=data["metadata"].copy(),
                    char_count=len(chunk_content),
                    token_count=self.count_tokens(chunk_content),
                    importance_score=self.CHUNK_IMPORTANCE["child_documents"]
                )
                chunks.append(chunk)

        return chunks

    def _create_requirements_chunks(self, data: Dict, parent_id: str) -> List[EnrichedChunk]:
        """
        Child Type B: Requirements (Y√™u c·∫ßu & ƒêi·ªÅu ki·ªán)

        PRESERVED: Original logic maintained
        """
        chunks = []

        yeu_cau = data["content"].get("y√™u_c·∫ßu_ƒëi·ªÅu_ki·ªán_th·ª±c_hi·ªán", "")
        doi_tuong = data["content"].get("ƒë·ªëi_t∆∞·ª£ng_th·ª±c_hi·ªán", "")

        # If no information
        if (not yeu_cau or yeu_cau in ["Kh√¥ng", "Kh√¥ng c√≥ th√¥ng tin"]) and \
           (not doi_tuong or doi_tuong in ["Kh√¥ng", "Kh√¥ng c√≥ th√¥ng tin"]):
            return chunks

        # Parent context
        parent_context = f"""[PARENT CONTEXT]
Th·ªß t·ª•c: {data['metadata'].get('t√™n_th·ªß_t·ª•c', '')}
M√£: {data['metadata'].get('m√£_th·ªß_t·ª•c', '')}
"""

        # Main content
        main_content = "[MAIN CONTENT]\n"

        if doi_tuong and doi_tuong not in ["Kh√¥ng", "Kh√¥ng c√≥ th√¥ng tin"]:
            main_content += f"ƒê·ªêI T∆Ø·ª¢NG ƒê∆Ø·ª¢C L√ÄM TH·ª¶ T·ª§C:\n{doi_tuong}\n\n"

        if yeu_cau and yeu_cau not in ["Kh√¥ng", "Kh√¥ng c√≥ th√¥ng tin"]:
            main_content += f"Y√äU C·∫¶U V√Ä ƒêI·ªÄU KI·ªÜN:\n{yeu_cau}\n"

        full_content = parent_context + "\n" + main_content
        token_count = self.count_tokens(full_content)

        # If < 768 tokens: single chunk
        if token_count <= self.CHUNK_PARAMS["child_requirements"]["max_tokens"]:
            chunk = EnrichedChunk(
                chunk_id=f"{data['thu_tuc_id']}_child_requirements_0",
                thu_tuc_id=data["thu_tuc_id"],
                chunk_type="child_requirements",
                chunk_tier="child",
                parent_chunk_id=parent_id,
                content=full_content.strip(),
                metadata=data["metadata"].copy(),
                char_count=len(full_content),
                token_count=token_count,
                importance_score=self.CHUNK_IMPORTANCE["child_requirements"]
            )
            chunks.append(chunk)
        else:
            # Split with overlap
            chunks_list = self._split_with_overlap(
                text=yeu_cau,
                max_tokens=self.CHUNK_PARAMS["child_requirements"]["max_tokens"],
                overlap_tokens=self.CHUNK_PARAMS["child_requirements"]["overlap"],
                prefix=parent_context + "\n[MAIN CONTENT]\nY√äU C·∫¶U V√Ä ƒêI·ªÄU KI·ªÜN:\n"
            )

            for i, chunk_text in enumerate(chunks_list):
                chunk = EnrichedChunk(
                    chunk_id=f"{data['thu_tuc_id']}_child_requirements_{i}",
                    thu_tuc_id=data["thu_tuc_id"],
                    chunk_type="child_requirements",
                    chunk_tier="child",
                    parent_chunk_id=parent_id,
                    content=chunk_text.strip(),
                    metadata=data["metadata"].copy(),
                    char_count=len(chunk_text),
                    token_count=self.count_tokens(chunk_text),
                    importance_score=self.CHUNK_IMPORTANCE["child_requirements"]
                )
                chunks.append(chunk)

        return chunks

    def _create_process_chunks(self, data: Dict, parent_id: str) -> List[EnrichedChunk]:
        """
        Child Type C: Process (Quy tr√¨nh & B∆∞·ªõc th·ª±c hi·ªán)

        PRESERVED: Original logic maintained
        IMPORTANT: Preserves both tr√¨nh_t·ª±_th·ª±c_hi·ªán AND c√°ch_th·ª©c_th·ª±c_hi·ªán
        """
        chunks = []

        trinh_tu = data["content"].get("tr√¨nh_t·ª±_th·ª±c_hi·ªán", "")
        cach_thuc = data["content"].get("c√°ch_th·ª©c_th·ª±c_hi·ªán", "")

        # If no information
        if (not trinh_tu or trinh_tu in ["", "Kh√¥ng c√≥ th√¥ng tin"]) and \
           (not cach_thuc or cach_thuc in ["", "Kh√¥ng c√≥ th√¥ng tin"]):
            return chunks

        # Parent context
        parent_context = f"""[PARENT CONTEXT]
Th·ªß t·ª•c: {data['metadata'].get('t√™n_th·ªß_t·ª•c', '')}
M√£: {data['metadata'].get('m√£_th·ªß_t·ª•c', '')}
"""

        # Main content
        main_content = "[MAIN CONTENT]\n"

        if trinh_tu and trinh_tu not in ["", "Kh√¥ng c√≥ th√¥ng tin"]:
            main_content += f"TR√åNH T·ª∞ TH·ª∞C HI·ªÜN:\n{trinh_tu}\n\n"

        if cach_thuc and cach_thuc not in ["", "Kh√¥ng c√≥ th√¥ng tin"]:
            main_content += f"C√ÅCH TH·ª®C TH·ª∞C HI·ªÜN:\n{cach_thuc}\n\n"

        # Time and location
        main_content += "TH·ªúI GIAN V√Ä ƒê·ªäA ƒêI·ªÇM:\n"

        hinh_thuc_nop = data["tables"].get("hinh_thuc_nop", [])
        if hinh_thuc_nop and len(hinh_thuc_nop) > 0:
            thoi_han = hinh_thuc_nop[0].get("thoi_han_giai_quyet", "")
            phi = hinh_thuc_nop[0].get("phi_le_phi", "")
            main_content += f"- Th·ªùi h·∫°n gi·∫£i quy·∫øt: {thoi_han}\n"
            main_content += f"- Ph√≠, l·ªá ph√≠: {phi}\n"

        dia_chi = data["content"].get("ƒë·ªãa_ch·ªâ_ti·∫øp_nh·∫≠n_hs", "")
        if dia_chi and dia_chi != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"- ƒê·ªãa ƒëi·ªÉm ti·∫øp nh·∫≠n: {dia_chi}\n"

        co_quan = data["content"].get("c∆°_quan_th·ª±c_hi·ªán", "")
        if co_quan:
            main_content += f"- C∆° quan th·ª±c hi·ªán: {co_quan}\n"

        full_content = parent_context + "\n" + main_content
        token_count = self.count_tokens(full_content)

        # If < 896 tokens: single chunk
        if token_count <= self.CHUNK_PARAMS["child_process"]["max_tokens"]:
            chunk = EnrichedChunk(
                chunk_id=f"{data['thu_tuc_id']}_child_process_0",
                thu_tuc_id=data["thu_tuc_id"],
                chunk_type="child_process",
                chunk_tier="child",
                parent_chunk_id=parent_id,
                content=full_content.strip(),
                metadata=data["metadata"].copy(),
                char_count=len(full_content),
                token_count=token_count,
                importance_score=self.CHUNK_IMPORTANCE["child_process"]
            )
            chunks.append(chunk)
        else:
            # Split with overlap
            combined_text = ""
            if trinh_tu:
                combined_text += f"TR√åNH T·ª∞ TH·ª∞C HI·ªÜN:\n{trinh_tu}\n\n"
            if cach_thuc:
                combined_text += f"C√ÅCH TH·ª®C TH·ª∞C HI·ªÜN:\n{cach_thuc}\n"

            chunks_list = self._split_with_overlap(
                text=combined_text,
                max_tokens=self.CHUNK_PARAMS["child_process"]["max_tokens"],
                overlap_tokens=self.CHUNK_PARAMS["child_process"]["overlap"],
                prefix=parent_context + "\n[MAIN CONTENT]\n"
            )

            for i, chunk_text in enumerate(chunks_list):
                chunk = EnrichedChunk(
                    chunk_id=f"{data['thu_tuc_id']}_child_process_{i}",
                    thu_tuc_id=data["thu_tuc_id"],
                    chunk_type="child_process",
                    chunk_tier="child",
                    parent_chunk_id=parent_id,
                    content=chunk_text.strip(),
                    metadata=data["metadata"].copy(),
                    char_count=len(chunk_text),
                    token_count=self.count_tokens(chunk_text),
                    importance_score=self.CHUNK_IMPORTANCE["child_process"]
                )
                chunks.append(chunk)

        return chunks

    def _create_legal_chunks(self, data: Dict, parent_id: str) -> List[EnrichedChunk]:
        """
        Child Type D: Legal Basis (CƒÉn c·ª© ph√°p l√Ω)

        PRESERVED: Original logic maintained
        """
        chunks = []
        can_cu_phap_ly = data["tables"].get("can_cu_phap_ly", [])

        if not can_cu_phap_ly or len(can_cu_phap_ly) == 0:
            return chunks

        # Parent context
        parent_context = f"""[PARENT CONTEXT]
Th·ªß t·ª•c: {data['metadata'].get('t√™n_th·ªß_t·ª•c', '')}
M√£: {data['metadata'].get('m√£_th·ªß_t·ª•c', '')}
"""

        # Build legal basis list
        main_content = "[MAIN CONTENT]\nCƒÇN C·ª® PH√ÅP L√ù:\n\n"

        for i, legal in enumerate(can_cu_phap_ly, 1):
            so_ky_hieu = legal.get("so_ky_hieu", "")
            trich_yeu = legal.get("trich_yeu", "")
            ngay_ban_hanh = legal.get("ngay_ban_hanh", "")  # FIXED: Add date
            co_quan_ban_hanh = legal.get("co_quan_ban_hanh", "")  # FIXED: Add issuing agency

            legal_text = f"{i}. {so_ky_hieu}\n"
            if trich_yeu:
                legal_text += f"   Tr√≠ch y·∫øu: {trich_yeu}\n"
            if ngay_ban_hanh:  # FIXED: Include date
                legal_text += f"   Ng√†y ban h√†nh: {ngay_ban_hanh}\n"
            if co_quan_ban_hanh:  # FIXED: Include issuing agency
                legal_text += f"   C∆° quan ban h√†nh: {co_quan_ban_hanh}\n"
            legal_text += "\n"

            main_content += legal_text

        full_content = parent_context + "\n" + main_content
        token_count = self.count_tokens(full_content)

        # If < 512 tokens: single chunk
        if token_count <= self.CHUNK_PARAMS["child_legal"]["max_tokens"]:
            chunk = EnrichedChunk(
                chunk_id=f"{data['thu_tuc_id']}_child_legal_0",
                thu_tuc_id=data["thu_tuc_id"],
                chunk_type="child_legal",
                chunk_tier="child",
                parent_chunk_id=parent_id,
                content=full_content.strip(),
                metadata=data["metadata"].copy(),
                char_count=len(full_content),
                token_count=token_count,
                importance_score=self.CHUNK_IMPORTANCE["child_legal"]
            )
            chunks.append(chunk)
        else:
            # Split into groups of 5 legal documents
            chunk_size = 5
            for i in range(0, len(can_cu_phap_ly), chunk_size):
                group = can_cu_phap_ly[i:i+chunk_size]

                group_content = "[MAIN CONTENT]\nCƒÇN C·ª® PH√ÅP L√ù:\n\n"
                for j, legal in enumerate(group, i+1):
                    so_ky_hieu = legal.get("so_ky_hieu", "")
                    trich_yeu = legal.get("trich_yeu", "")
                    ngay_ban_hanh = legal.get("ngay_ban_hanh", "")  # FIXED: Add date
                    co_quan_ban_hanh = legal.get("co_quan_ban_hanh", "")  # FIXED: Add issuing agency

                    legal_text = f"{j}. {so_ky_hieu}\n"
                    if trich_yeu:
                        legal_text += f"   Tr√≠ch y·∫øu: {trich_yeu}\n"
                    if ngay_ban_hanh:  # FIXED: Include date
                        legal_text += f"   Ng√†y ban h√†nh: {ngay_ban_hanh}\n"
                    if co_quan_ban_hanh:  # FIXED: Include issuing agency
                        legal_text += f"   C∆° quan ban h√†nh: {co_quan_ban_hanh}\n"
                    legal_text += "\n"

                    group_content += legal_text

                chunk_content = parent_context + "\n" + group_content

                chunk = EnrichedChunk(
                    chunk_id=f"{data['thu_tuc_id']}_child_legal_{i//chunk_size}",
                    thu_tuc_id=data["thu_tuc_id"],
                    chunk_type="child_legal",
                    chunk_tier="child",
                    parent_chunk_id=parent_id,
                    content=chunk_content.strip(),
                    metadata=data["metadata"].copy(),
                    char_count=len(chunk_content),
                    token_count=self.count_tokens(chunk_content),
                    importance_score=self.CHUNK_IMPORTANCE["child_legal"]
                )
                chunks.append(chunk)

        return chunks

    # ========== NEW CHUNK METHODS ==========

    def _create_fees_timing_chunks(self, data: Dict, parent_id: str) -> List[EnrichedChunk]:
        """
        NEW Child Type E: Fees & Timing (Ph√≠ l·ªá ph√≠ + Th·ªùi h·∫°n + H√¨nh th·ª©c n·ªôp)

        CRITICAL: Includes ALL items from hinh_thuc_nop table, not just first one!
        This consolidates all submission methods, fees, and timing information.
        """
        chunks = []
        hinh_thuc_nop = data["tables"].get("hinh_thuc_nop", [])

        # If no timing/fees information
        if not hinh_thuc_nop or len(hinh_thuc_nop) == 0:
            return chunks

        # Parent context
        parent_context = f"""[PARENT CONTEXT]
Th·ªß t·ª•c: {data['metadata'].get('t√™n_th·ªß_t·ª•c', '')}
M√£: {data['metadata'].get('m√£_th·ªß_t·ª•c', '')}
"""

        # Build comprehensive fees & timing information
        main_content = "[MAIN CONTENT]\nPH√ç L·ªÜ PH√ç V√Ä TH·ªúI H·∫†N GI·∫¢I QUY·∫æT:\n\n"

        # IMPORTANT: Process ALL submission methods, not just the first one
        for i, form in enumerate(hinh_thuc_nop, 1):
            hinh_thuc = form.get("hinh_thuc", "")
            thoi_han = form.get("thoi_han_giai_quyet", "")
            phi = form.get("phi_le_phi", "")
            mo_ta = form.get("mo_ta", "")  # FIXED: Add mo_ta field from table

            main_content += f"{i}. H√åNH TH·ª®C: {hinh_thuc}\n"
            if thoi_han:
                main_content += f"   - Th·ªùi h·∫°n gi·∫£i quy·∫øt: {thoi_han}\n"
            if phi:
                main_content += f"   - Ph√≠, l·ªá ph√≠: {phi}\n"
            if mo_ta:  # FIXED: Include mo_ta description
                main_content += f"   - M√¥ t·∫£: {mo_ta}\n"
            main_content += "\n"

        # Add submission address if available
        dia_chi = data["content"].get("ƒë·ªãa_ch·ªâ_ti·∫øp_nh·∫≠n_hs", "")
        if dia_chi and dia_chi != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"ƒê·ªäA ƒêI·ªÇM TI·∫æP NH·∫¨N H·ªí S∆†:\n{dia_chi}\n"

        full_content = parent_context + "\n" + main_content
        token_count = self.count_tokens(full_content)

        # Create chunk (usually fits in 512 tokens)
        chunk = EnrichedChunk(
            chunk_id=f"{data['thu_tuc_id']}_child_fees_timing_0",
            thu_tuc_id=data["thu_tuc_id"],
            chunk_type="child_fees_timing",
            chunk_tier="child",
            parent_chunk_id=parent_id,
            content=full_content.strip(),
            metadata=data["metadata"].copy(),
            char_count=len(full_content),
            token_count=token_count,
            importance_score=self.CHUNK_IMPORTANCE["child_fees_timing"]
        )
        chunks.append(chunk)

        return chunks

    def _create_agencies_chunks(self, data: Dict, parent_id: str) -> List[EnrichedChunk]:
        """
        NEW Child Type F: Agencies (T·∫•t c·∫£ c∆° quan + ƒë·ªãa ch·ªâ)

        CRITICAL: Includes ALL agency-related fields:
        - C∆° quan th·ª±c hi·ªán
        - C∆° quan c√≥ th·∫©m quy·ªÅn
        - C∆° quan ph·ªëi h·ª£p
        - ƒê·ªãa ch·ªâ ti·∫øp nh·∫≠n h·ªì s∆°
        - K·∫øt qu·∫£ th·ª±c hi·ªán (what agency provides)
        """
        chunks = []
        content_data = data["content"]

        # Extract all agency fields
        co_quan_thuc_hien = content_data.get("c∆°_quan_th·ª±c_hi·ªán", "")
        co_quan_tham_quyen = content_data.get("c∆°_quan_c√≥_th·∫©m_quy·ªÅn", "")
        co_quan_phoi_hop = content_data.get("c∆°_quan_ph·ªëi_h·ª£p", "")
        dia_chi = content_data.get("ƒë·ªãa_ch·ªâ_ti·∫øp_nh·∫≠n_hs", "")
        ket_qua = content_data.get("k·∫øt_qu·∫£_th·ª±c_hi·ªán", "")

        # If no agency information at all
        if not any([co_quan_thuc_hien, co_quan_tham_quyen, co_quan_phoi_hop, dia_chi]):
            return chunks

        # Parent context
        parent_context = f"""[PARENT CONTEXT]
Th·ªß t·ª•c: {data['metadata'].get('t√™n_th·ªß_t·ª•c', '')}
M√£: {data['metadata'].get('m√£_th·ªß_t·ª•c', '')}
"""

        # Build comprehensive agency information
        main_content = "[MAIN CONTENT]\nC√ÅC C∆† QUAN LI√äN QUAN:\n\n"

        # Implementing agency
        if co_quan_thuc_hien and co_quan_thuc_hien != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"1. C∆† QUAN TH·ª∞C HI·ªÜN:\n{co_quan_thuc_hien}\n\n"

        # Authorized agency
        if co_quan_tham_quyen and co_quan_tham_quyen != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"2. C∆† QUAN C√ì TH·∫®M QUY·ªÄN:\n{co_quan_tham_quyen}\n\n"

        # Coordinating agencies
        if co_quan_phoi_hop and co_quan_phoi_hop != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"3. C∆† QUAN PH·ªêI H·ª¢P:\n{co_quan_phoi_hop}\n\n"

        # Submission address
        if dia_chi and dia_chi != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"ƒê·ªäA CH·ªà TI·∫æP NH·∫¨N H·ªí S∆†:\n{dia_chi}\n\n"

        # Result (what you get from which agency)
        if ket_qua and ket_qua != "Kh√¥ng c√≥ th√¥ng tin":
            main_content += f"K·∫æT QU·∫¢ TH·ª∞C HI·ªÜN:\n{ket_qua}\n"

        full_content = parent_context + "\n" + main_content
        token_count = self.count_tokens(full_content)

        # Create chunk (usually fits in 640 tokens)
        chunk = EnrichedChunk(
            chunk_id=f"{data['thu_tuc_id']}_child_agencies_0",
            thu_tuc_id=data["thu_tuc_id"],
            chunk_type="child_agencies",
            chunk_tier="child",
            parent_chunk_id=parent_id,
            content=full_content.strip(),
            metadata=data["metadata"].copy(),
            char_count=len(full_content),
            token_count=token_count,
            importance_score=self.CHUNK_IMPORTANCE["child_agencies"]
        )
        chunks.append(chunk)

        return chunks

    # ========== ENRICHMENT METHODS ==========

    def _enrich_all_chunks(
        self,
        chunks: List[EnrichedChunk],
        parent_chunk: EnrichedChunk,
        thu_tuc_data: Dict
    ) -> List[EnrichedChunk]:
        """
        Enrich all chunks with:
        1. Parent context (first 200 chars of parent)
        2. Breadcrumb (Lƒ©nh v·ª±c > Procedure > Chunk type)
        3. Sibling chunk IDs (all other chunks from same procedure)
        4. Complexity level (based on content length)
        5. Related procedure IDs (from ProcedureGraph if available)
        """
        # Extract parent context (first 200 chars)
        parent_context_text = parent_chunk.content[:200]

        # Build sibling map
        chunk_ids = [c.chunk_id for c in chunks]

        # Get related procedures from graph (if available)
        related_procedure_ids = []
        if self.procedure_graph:
            thu_tuc_id = thu_tuc_data["thu_tuc_id"]

            # Get top 10 related procedures (mixed from all types)
            # Priority: same_domain > related_legal > similar > sequential
            related = self.procedure_graph.get_related_procedures(
                thu_tuc_id,
                relationship_types=None,  # All types
                min_strength=0.5,  # Only strong relationships
                max_results=10
            )
            related_procedure_ids = [rel_id for rel_id, _ in related]

        # Enrich each chunk
        for chunk in chunks:
            # 1. Add parent context
            chunk.parent_context = parent_context_text

            # 2. Build breadcrumb
            chunk.breadcrumb = self._build_breadcrumb(thu_tuc_data, chunk)

            # 3. Add sibling chunk IDs (all other chunks from same procedure)
            chunk.sibling_chunk_ids = [cid for cid in chunk_ids if cid != chunk.chunk_id]

            # 4. Determine complexity level
            chunk.complexity_level = self._calculate_complexity(chunk)

            # 5. Add related procedure IDs (from graph)
            chunk.related_procedure_ids = related_procedure_ids

        return chunks

    def _build_breadcrumb(self, thu_tuc_data: Dict, chunk: EnrichedChunk) -> str:
        """
        Build breadcrumb path: Lƒ©nh v·ª±c > Procedure name > Chunk type

        Example: "H·ªô t·ªãch > ƒêƒÉng k√Ω k·∫øt h√¥n > Documents"
        """
        linh_vuc = thu_tuc_data['metadata'].get('lƒ©nh_v·ª±c', 'Unknown')
        ten_thu_tuc = thu_tuc_data['metadata'].get('t√™n_th·ªß_t·ª•c', 'Unknown')

        # Map chunk type to Vietnamese readable name
        chunk_type_names = {
            "parent_overview": "T·ªïng quan",
            "child_documents": "H·ªì s∆°",
            "child_requirements": "Y√™u c·∫ßu",
            "child_process": "Quy tr√¨nh",
            "child_legal": "CƒÉn c·ª© ph√°p l√Ω",
            "child_fees_timing": "Ph√≠ & Th·ªùi h·∫°n",
            "child_agencies": "C∆° quan"
        }

        chunk_name = chunk_type_names.get(chunk.chunk_type, chunk.chunk_type)

        # Truncate if too long
        if len(ten_thu_tuc) > 50:
            ten_thu_tuc = ten_thu_tuc[:47] + "..."

        breadcrumb = f"{linh_vuc} > {ten_thu_tuc} > {chunk_name}"

        return breadcrumb

    def _calculate_complexity(self, chunk: EnrichedChunk) -> str:
        """
        Calculate complexity level based on content length

        - simple: < 500 chars
        - medium: 500-1500 chars
        - complex: > 1500 chars
        """
        char_count = chunk.char_count

        if char_count < 500:
            return "simple"
        elif char_count < 1500:
            return "medium"
        else:
            return "complex"

    # ========== UTILITY METHODS ==========

    def _split_with_overlap(self, text: str, max_tokens: int, overlap_tokens: int, prefix: str = "") -> List[str]:
        """
        Split text into chunks with overlap

        PRESERVED: Original logic maintained
        """
        # Encode full text
        tokens = self.encoder.encode(text)

        # Account for prefix tokens
        prefix_tokens = self.encoder.encode(prefix) if prefix else []
        available_tokens = max_tokens - len(prefix_tokens)

        chunks = []
        start = 0

        while start < len(tokens):
            # Get chunk
            end = min(start + available_tokens, len(tokens))
            chunk_tokens = tokens[start:end]

            # Decode
            chunk_text = self.encoder.decode(chunk_tokens)

            # Add prefix
            full_chunk = prefix + chunk_text if prefix else chunk_text
            chunks.append(full_chunk)

            # Move start with overlap
            if end >= len(tokens):
                break
            start = end - overlap_tokens

        return chunks

    def chunk_all_files(self, input_dir: Path, output_dir: Path):
        """
        Chunk all JSON files in directory

        Enhanced to show new chunk types in statistics
        """
        json_files = list(input_dir.glob("*.json"))

        print(f"üîÑ B·∫Øt ƒë·∫ßu chunking v·ªõi GraphChunker (enhanced)...")
        print(f"   - {len(json_files)} files")
        print(f"   - 6 chunk types (4 existing + 2 NEW)")
        print()

        all_chunks = []
        total_chunks_count = 0

        for i, file_path in enumerate(json_files, 1):
            print(f"\r‚è≥ Chunking: {i}/{len(json_files)}", end='', flush=True)

            # Load JSON
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # Chunk with enrichment
            chunks = self.chunk_thu_tuc(data)
            all_chunks.extend(chunks)
            total_chunks_count += len(chunks)

        print("\n")

        # Save all chunks to JSON
        output_dir.mkdir(parents=True, exist_ok=True)
        chunks_file = output_dir / "all_chunks_enriched.json"

        # Convert chunks to dict
        chunks_dict = [asdict(chunk) for chunk in all_chunks]

        with open(chunks_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_dict, f, ensure_ascii=False, indent=2)

        # Print detailed statistics
        print("=" * 80)
        print("K·∫æT QU·∫¢ CHUNKING V·ªöI ENRICHMENT")
        print("=" * 80)
        print(f"üìä T·ªïng s·ªë th·ªß t·ª•c: {len(json_files)}")
        print(f"üì¶ T·ªïng s·ªë chunks: {total_chunks_count}")
        print(f"   - Parent chunks: {sum(1 for c in all_chunks if c.chunk_tier == 'parent')}")
        print(f"   - Child chunks: {sum(1 for c in all_chunks if c.chunk_tier == 'child')}")
        print()
        print(f"Chi ti·∫øt child chunks:")
        print(f"   - Documents: {sum(1 for c in all_chunks if c.chunk_type == 'child_documents')}")
        print(f"   - Requirements: {sum(1 for c in all_chunks if c.chunk_type == 'child_requirements')}")
        print(f"   - Process: {sum(1 for c in all_chunks if c.chunk_type == 'child_process')}")
        print(f"   - Legal: {sum(1 for c in all_chunks if c.chunk_type == 'child_legal')}")
        print(f"   - Fees & Timing: {sum(1 for c in all_chunks if c.chunk_type == 'child_fees_timing')} ‚≠ê NEW")
        print(f"   - Agencies: {sum(1 for c in all_chunks if c.chunk_type == 'child_agencies')} ‚≠ê NEW")
        print()
        print(f"‚ú® Enrichment:")
        print(f"   - All chunks have parent context (first 200 chars)")
        print(f"   - All chunks have breadcrumb paths")
        print(f"   - All chunks have sibling IDs")
        print(f"   - All chunks have importance scores")
        print()
        print(f"üíæ Enriched chunks saved: {chunks_file}")
        print("=" * 80)

        # Generate statistics
        self._generate_chunking_stats(all_chunks, output_dir)

        return all_chunks

    def _generate_chunking_stats(self, chunks: List[EnrichedChunk], output_dir: Path):
        """Generate enhanced chunking statistics"""
        stats = {
            "total_chunks": len(chunks),
            "by_tier": {},
            "by_type": {},
            "token_stats": {
                "min": min(c.token_count for c in chunks),
                "max": max(c.token_count for c in chunks),
                "avg": sum(c.token_count for c in chunks) / len(chunks)
            },
            "enrichment_stats": {
                "chunks_with_parent_context": sum(1 for c in chunks if c.parent_context),
                "chunks_with_breadcrumb": sum(1 for c in chunks if c.breadcrumb),
                "chunks_with_siblings": sum(1 for c in chunks if c.sibling_chunk_ids),
                "avg_siblings_per_chunk": sum(len(c.sibling_chunk_ids) for c in chunks) / len(chunks)
            }
        }

        # By tier
        for tier in ["parent", "child"]:
            tier_chunks = [c for c in chunks if c.chunk_tier == tier]
            stats["by_tier"][tier] = {
                "count": len(tier_chunks),
                "avg_tokens": sum(c.token_count for c in tier_chunks) / len(tier_chunks) if tier_chunks else 0
            }

        # By type (including new types)
        chunk_types = set(c.chunk_type for c in chunks)
        for chunk_type in chunk_types:
            type_chunks = [c for c in chunks if c.chunk_type == chunk_type]
            stats["by_type"][chunk_type] = {
                "count": len(type_chunks),
                "avg_tokens": sum(c.token_count for c in type_chunks) / len(type_chunks) if type_chunks else 0,
                "max_tokens": max(c.token_count for c in type_chunks) if type_chunks else 0,
                "avg_importance": sum(c.importance_score for c in type_chunks) / len(type_chunks) if type_chunks else 0
            }

        # Save stats
        stats_file = output_dir / "enriched_chunking_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, ensure_ascii=False, indent=2)

        print(f"üìà Enhanced statistics saved: {stats_file}")


def main():
    """Main function"""
    # Paths
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    data_dir = project_root / "data"
    extracted_dir = data_dir / "extracted"
    chunks_dir = data_dir / "chunks_v2"  # NEW directory for enriched chunks

    if not extracted_dir.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c extracted: {extracted_dir}")
        print("   H√£y ch·∫°y extract_documents.py tr∆∞·ªõc!")
        return

    # Create enhanced chunker
    print("üöÄ Kh·ªüi t·∫°o GraphChunker (Enhanced Hierarchical Chunker)")
    print("=" * 80)
    chunker = GraphChunker()

    # Chunk all files with enrichment
    chunks = chunker.chunk_all_files(extracted_dir, chunks_dir)

    return chunks


if __name__ == "__main__":
    main()
