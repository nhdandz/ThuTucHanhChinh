#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Answer Generator - Phase 4: Generation & Answer Synthesis
Uses Ollama LLM to generate context-based answers with source citation
"""

import sys
import json
import requests
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from datetime import datetime

# Import configuration
try:
    from backend.config import settings
except ImportError:
    # Fallback if backend module is not in path
    settings = None

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')


@dataclass
class SourceCitation:
    """Source citation for answer"""
    chunk_id: str
    thu_tuc_name: str
    thu_tuc_code: str
    chunk_type: str
    relevance_score: float
    content_snippet: str  # First 200 chars for reference


@dataclass
class GeneratedAnswer:
    """Complete answer with metadata"""
    question: str
    answer: str  # Natural language answer
    structured_data: Dict  # JSON structured answer
    sources: List[SourceCitation]
    confidence: float
    intent: str
    timestamp: str
    metadata: Dict


class OllamaAnswerGenerator:
    """
    Answer Generator using Ollama LLM

    Features:
    1. Context-only answers (no hallucination)
    2. Hybrid output: JSON + Natural Language
    3. Source citation with chunk_id
    4. Confidence scoring
    """

    def __init__(
        self,
        model_name: str = "qwen3:8b",
        ollama_url: str = "http://localhost:11434"
    ):
        """
        Initialize answer generator

        Args:
            model_name: Ollama LLM model
            ollama_url: Ollama server URL
        """
        print(f"ğŸ”„ Initializing Answer Generator")
        print(f"   Model: {model_name}")
        print(f"   Server: {ollama_url}")

        self.model_name = model_name
        self.ollama_url = ollama_url
        self.generate_endpoint = f"{ollama_url}/api/generate"

        print(f"âœ… Answer Generator initialized!")

    def _call_ollama(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: float = 0.1
    ) -> str:
        """
        Call Ollama LLM API

        Args:
            prompt: User prompt
            system: System prompt
            temperature: Sampling temperature (low for factual answers)

        Returns:
            Generated text
        """
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature,
                "top_p": 0.9,
                "top_k": 40
            }
        }

        if system:
            payload["system"] = system

        try:
            response = requests.post(
                self.generate_endpoint,
                json=payload,
                timeout=120  # Longer timeout for generation
            )

            if response.status_code != 200:
                raise Exception(f"Ollama API error: {response.status_code}")

            return response.json()["response"].strip()
        except Exception as e:
            print(f"âš ï¸ Ollama API call failed: {e}")
            return ""

    def _extract_sources(self, retrieved_chunks: List[Dict]) -> List[SourceCitation]:
        """
        Extract source citations from retrieved chunks

        Args:
            retrieved_chunks: List of retrieved chunk dicts

        Returns:
            List of SourceCitation objects
        """
        sources = []

        for chunk in retrieved_chunks:
            # Extract metadata
            metadata = chunk.get("metadata", {})
            content = chunk.get("content", "")

            # Create citation (check top-level fields first, fallback to nested metadata)
            citation = SourceCitation(
                chunk_id=chunk.get("chunk_id", "N/A"),
                thu_tuc_name=chunk.get("tÃªn_thá»§_tá»¥c", metadata.get("tÃªn_thá»§_tá»¥c", "N/A")),
                thu_tuc_code=chunk.get("mÃ£_thá»§_tá»¥c", metadata.get("mÃ£_thá»§_tá»¥c", "N/A")),
                chunk_type=chunk.get("chunk_type", "N/A"),
                relevance_score=chunk.get("final_score", 0.0),
                content_snippet=content[:200] + "..." if len(content) > 200 else content
            )

            sources.append(citation)

        return sources

    def _generate_structured_answer(
        self,
        question: str,
        context: str,
        intent: str
    ) -> Dict:
        """
        Generate structured JSON answer based on intent

        Args:
            question: User question
            context: Retrieved context
            intent: Question intent

        Returns:
            Structured dictionary answer
        """
        system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» thá»§ tá»¥c hÃ nh chÃ­nh Viá»‡t Nam.

NHIá»†M Vá»¤:
- TrÃ­ch xuáº¥t thÃ´ng tin cÃ³ cáº¥u trÃºc tá»« context Ä‘Æ°á»£c cung cáº¥p
- KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin
- Chá»‰ tráº£ lá»i dá»±a HOÃ€N TOÃ€N trÃªn context
- Náº¿u context khÃ´ng chá»©a thÃ´ng tin, tráº£ vá» giÃ¡ trá»‹ rá»—ng
- **Báº®T BUá»˜C: Táº¥t cáº£ giÃ¡ trá»‹ trong JSON pháº£i báº±ng TIáº¾NG VIá»†T**

QUAN TRá»ŒNG:
- Chá»‰ tráº£ vá» JSON, KHÃ”NG giáº£i thÃ­ch
- JSON pháº£i valid vÃ  cÃ³ thá»ƒ parse Ä‘Æ°á»£c
- Táº¥t cáº£ text trong JSON pháº£i báº±ng TIáº¾NG VIá»†T
"""

        # Intent-specific JSON schema prompts
        intent_schemas = {
            "documents": """
TrÃ­ch xuáº¥t danh sÃ¡ch giáº¥y tá» cáº§n ná»™p.
Tráº£ vá» JSON:
{
  "ho_so_bao_gom": ["giáº¥y tá» 1", "giáº¥y tá» 2", ...],
  "so_ban": {"giáº¥y tá» 1": "sá»‘ báº£n", ...},
  "ghi_chu": "..."
}
""",
            "requirements": """
TrÃ­ch xuáº¥t Ä‘iá»u kiá»‡n vÃ  yÃªu cáº§u.
Tráº£ vá» JSON:
{
  "doi_tuong": "mÃ´ táº£ Ä‘á»‘i tÆ°á»£ng Ä‘Æ°á»£c lÃ m thá»§ tá»¥c",
  "dieu_kien": ["Ä‘iá»u kiá»‡n 1", "Ä‘iá»u kiá»‡n 2", ...],
  "yeu_cau": ["yÃªu cáº§u 1", "yÃªu cáº§u 2", ...]
}
""",
            "process": """
TrÃ­ch xuáº¥t quy trÃ¬nh thá»±c hiá»‡n.
Tráº£ vá» JSON:
{
  "cac_buoc": [
    {"buoc": 1, "mo_ta": "..."},
    {"buoc": 2, "mo_ta": "..."}
  ],
  "ghi_chu": "..."
}
""",
            "legal": """
TrÃ­ch xuáº¥t cÄƒn cá»© phÃ¡p lÃ½.
Tráº£ vá» JSON:
{
  "can_cu_phap_ly": ["vÄƒn báº£n 1", "vÄƒn báº£n 2", ...],
  "ghi_chu": "..."
}
""",
            "timeline": """
TrÃ­ch xuáº¥t thÃ´ng tin thá»i gian.
Tráº£ vá» JSON:
{
  "thoi_han_giai_quyet": "...",
  "thoi_gian_tiep_nhan": "...",
  "ghi_chu": "..."
}
""",
            "fees": """
TrÃ­ch xuáº¥t thÃ´ng tin phÃ­.
Tráº£ vá» JSON:
{
  "le_phi": "...",
  "phi_khac": "...",
  "ghi_chu": "..."
}
""",
            "overview": """
TrÃ­ch xuáº¥t thÃ´ng tin tá»•ng quan.
Tráº£ vá» JSON:
{
  "ten_thu_tuc": "...",
  "ma_thu_tuc": "...",
  "linh_vuc": "...",
  "trich_yeu": "...",
  "co_quan_thuc_hien": "..."
}
"""
        }

        # Get schema for intent
        schema_prompt = intent_schemas.get(intent, intent_schemas["overview"])

        prompt = f"""CÃ¢u há»i: {question}

Context tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u:
{context}

{schema_prompt}

Chá»‰ tráº£ vá» JSON, khÃ´ng giáº£i thÃ­ch:"""

        try:
            response = self._call_ollama(prompt, system=system_prompt, temperature=0.1)

            # Extract JSON from response - try multiple strategies
            json_str = None

            # Strategy 1: Try to find JSON in markdown code blocks
            if "```json" in response:
                start_marker = response.find("```json") + 7
                end_marker = response.find("```", start_marker)
                if end_marker > start_marker:
                    json_str = response[start_marker:end_marker].strip()

            # Strategy 2: Find first complete JSON object
            if not json_str:
                start = response.find("{")
                if start != -1:
                    # Find matching closing brace
                    brace_count = 0
                    for i in range(start, len(response)):
                        if response[i] == "{":
                            brace_count += 1
                        elif response[i] == "}":
                            brace_count -= 1
                            if brace_count == 0:
                                json_str = response[start:i+1]
                                break

            # Try to parse JSON
            if json_str:
                structured_data = json.loads(json_str)
                return structured_data
            else:
                print(f"âš ï¸ No valid JSON found in LLM response")

        except json.JSONDecodeError as e:
            print(f"âš ï¸ Failed to parse JSON: {e}")
            print(f"   Response preview: {response[:200]}...")
        except Exception as e:
            print(f"âš ï¸ Failed to generate structured answer: {e}")

        # Return empty structure on failure
        return {}

    def _generate_natural_language_answer(
        self,
        question: str,
        context: str,
        intent: str,
        structured_data: Dict
    ) -> str:
        """
        Generate natural language answer

        Args:
            question: User question
            context: Retrieved context
            intent: Question intent
            structured_data: Previously generated structured data

        Returns:
            Natural language answer string
        """
        system_prompt = """Báº¡n lÃ  trá»£ lÃ½ AI chuyÃªn vá» thá»§ tá»¥c hÃ nh chÃ­nh Viá»‡t Nam.

NGUYÃŠN Táº®C QUAN TRá»ŒNG:
1. CHá»ˆ tráº£ lá»i dá»±a trÃªn CONTEXT Ä‘Æ°á»£c cung cáº¥p
2. KHÃ”NG bá»‹a Ä‘áº·t thÃ´ng tin khÃ´ng cÃ³ trong context
3. Náº¿u context khÃ´ng cÃ³ thÃ´ng tin, hÃ£y nÃ³i rÃµ "ThÃ´ng tin nÃ y khÃ´ng cÃ³ trong tÃ i liá»‡u"
4. Tráº£ lá»i CHÃNH XÃC, SÃšC TÃCH, Dá»„ HIá»‚U
5. Sá»­ dá»¥ng ngÃ´n ngá»¯ tá»± nhiÃªn, thÃ¢n thiá»‡n
6. **Báº®T BUá»˜C: Tráº£ lá»i HOÃ€N TOÃ€N báº±ng TIáº¾NG VIá»†T, KHÃ”NG Ä‘Æ°á»£c dÃ¹ng tiáº¿ng Anh**

YÃŠU Cáº¦U HÃ€NH VI:
- Náº¿u KHÃ”NG TÃŒM THáº¤Y thÃ´ng tin trong context, hÃ£y nÃ³i: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u. Báº¡n cÃ³ thá»ƒ cung cáº¥p thÃªm chi tiáº¿t (tÃªn thá»§ tá»¥c, lÄ©nh vá»±c, hoáº·c mÃ£ thá»§ tá»¥c) Ä‘á»ƒ tÃ´i tÃ¬m kiáº¿m chÃ­nh xÃ¡c hÆ¡n khÃ´ng?"

Cáº¤U TRÃšC TRáº¢ Lá»œI (QUAN TRá»ŒNG - Ãp dá»¥ng cho cÃ¢u há»i cÃ³/khÃ´ng, Ä‘á»§ Ä‘iá»u kiá»‡n/khÃ´ng Ä‘á»§):
1. **Káº¾T LUáº¬N TRá»°C TIáº¾P** ngay á»Ÿ Ä‘áº§u (CÃ³/KhÃ´ng, Äá»§/KhÃ´ng Ä‘á»§, ÄÆ°á»£c/KhÃ´ng Ä‘Æ°á»£c)
2. **LÃ DO CHÃNH** (1-2 cÃ¢u giáº£i thÃ­ch ngáº¯n gá»n táº¡i sao)
3. **CHI TIáº¾T QUY Äá»ŠNH** (náº¿u cáº§n thiáº¿t Ä‘á»ƒ lÃ m rÃµ)

VÃ­ dá»¥ tá»‘t:
"Dá»±a trÃªn quy Ä‘á»‹nh hiá»‡n hÃ nh, trÆ°á»ng há»£p cá»§a báº¡n KHÃ”NG Ä‘á»§ Ä‘iá»u kiá»‡n Ä‘á»ƒ hÆ°á»Ÿng cháº¿ Ä‘á»™ nÃ y.

LÃ½ do: Máº·c dÃ¹ báº¡n Ä‘Ã¡p á»©ng Ä‘iá»u kiá»‡n vá» thá»i gian phá»¥c vá»¥ (22 nÄƒm), nhÆ°ng báº¡n Ä‘ang hÆ°á»Ÿng cháº¿ Ä‘á»™ máº¥t sá»©c lao Ä‘á»™ng hÃ ng thÃ¡ng - Ä‘Ã¢y lÃ  Ä‘iá»u kiá»‡n loáº¡i trá»« theo quy Ä‘á»‹nh.

Chi tiáº¿t quy Ä‘á»‹nh:
- Äá»‘i tÆ°á»£ng: QuÃ¢n nhÃ¢n cÃ³ tá»« 20 nÄƒm phá»¥c vá»¥ trá»Ÿ lÃªn
- Äiá»u kiá»‡n loáº¡i trá»«: KhÃ´ng Ä‘Æ°á»£c Ä‘ang hÆ°á»Ÿng cháº¿ Ä‘á»™ máº¥t sá»©c lao Ä‘á»™ng..."

VÃ­ dá»¥ xáº¥u (trÃ¡nh):
"Theo quy Ä‘á»‹nh, Ä‘á»‘i tÆ°á»£ng hÆ°á»Ÿng cháº¿ Ä‘á»™ gá»“m cÃ³... Äiá»u kiá»‡n loáº¡i trá»« gá»“m cÃ³..." (liá»‡t kÃª chung chung, khÃ´ng káº¿t luáº­n trá»±c tiáº¿p)

Äá»ŠNH Dáº NG TRáº¢ Lá»œI:
- ÄI THáº²NG VÃ€O Káº¾T LUáº¬N trÆ°á»›c, giáº£i thÃ­ch sau
- Sáº¯p xáº¿p thÃ´ng tin theo danh sÃ¡ch náº¿u cÃ³ nhiá»u má»¥c
- Káº¿t thÃºc báº±ng ghi chÃº quan trá»ng (náº¿u cÃ³)
"""

        prompt = f"""CÃ¢u há»i: "{question}"

Context tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u:
{context}

Dá»¯ liá»‡u cÃ³ cáº¥u trÃºc Ä‘Ã£ trÃ­ch xuáº¥t:
{json.dumps(structured_data, ensure_ascii=False, indent=2)}

HÃ£y tráº£ lá»i cÃ¢u há»i báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn, dá»… hiá»ƒu.
Tráº£ lá»i:"""
        print("\n[DEBUG] Natural Language Answer Prompt:", prompt, "\n")
        answer = self._call_ollama(prompt, system=system_prompt, temperature=0.2)

        if not answer:
            answer = "Xin lá»—i, tÃ´i khÃ´ng thá»ƒ táº¡o cÃ¢u tráº£ lá»i tá»« thÃ´ng tin cÃ³ sáºµn."

        return answer

    def generate(
        self,
        question: str,
        intent: str,
        context: str,
        retrieved_chunks: List[Dict],
        confidence: float,
        metadata: Dict,
        enable_structured_output: Optional[bool] = None  # NEW: Override for intent-based config
    ) -> GeneratedAnswer:
        """
        Main generation method - creates complete answer with sources

        Args:
            question: User question
            intent: Question intent
            context: Retrieved context
            retrieved_chunks: List of retrieved chunks
            confidence: Retrieval confidence score
            metadata: Additional metadata
            enable_structured_output: Override for structured output (None = use settings default)

        Returns:
            GeneratedAnswer object with complete answer
        """
        print("\n" + "=" * 80)
        print("ANSWER GENERATION")
        print("=" * 80)
        print(f"\nQuestion: {question}")
        print(f"Intent: {intent}")
        print(f"Confidence: {confidence:.2f}")
        print()

        # Step 1: Extract sources
        print("[Step 1/3] Extracting source citations...")
        sources = self._extract_sources(retrieved_chunks)
        print(f"   âœ“ {len(sources)} sources extracted")

        # Handle case when no context is available
        if not context or not context.strip():
            print("   âš ï¸ No context available - generating fallback response")
            structured_data = {}
            natural_answer = "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin vá» váº¥n Ä‘á» nÃ y trong cÆ¡ sá»Ÿ dá»¯ liá»‡u. Báº¡n cÃ³ thá»ƒ cung cáº¥p thÃªm chi tiáº¿t (tÃªn thá»§ tá»¥c, lÄ©nh vá»±c, hoáº·c mÃ£ thá»§ tá»¥c) Ä‘á»ƒ tÃ´i tÃ¬m kiáº¿m chÃ­nh xÃ¡c hÆ¡n khÃ´ng?"
        else:
            # Use parameter override if provided, else use settings default
            if enable_structured_output is None:
                # No override - use settings default
                enable_structured = True
                if settings and hasattr(settings, 'enable_structured_output'):
                    enable_structured = settings.enable_structured_output
            else:
                # Override provided by intent-based config
                enable_structured = enable_structured_output

            # Step 2: Generate structured answer (if enabled)
            if enable_structured:
                print("[Step 2/3] Generating structured answer (JSON)...")
                structured_data = self._generate_structured_answer(question, context, intent)
                print(f"   âœ“ Structured data generated")
            else:
                print("[Step 2/3] Structured output disabled - skipping JSON generation")
                structured_data = {}

            # Step 3: Generate natural language answer
            print("[Step 3/3] Generating natural language answer...")
            natural_answer = self._generate_natural_language_answer(
                question, context, intent, structured_data
            )
            print(f"   âœ“ Natural language answer generated ({len(natural_answer)} chars)")

        # Create final answer object
        answer = GeneratedAnswer(
            question=question,
            answer=natural_answer,
            structured_data=structured_data,
            sources=sources,
            confidence=confidence,
            intent=intent,
            timestamp=datetime.now().isoformat(),
            metadata=metadata
        )

        print("\n" + "=" * 80)
        print("âœ… ANSWER GENERATION COMPLETE")
        print("=" * 80)

        return answer

    def format_answer_for_display(self, answer: GeneratedAnswer) -> str:
        """
        Format answer for user-friendly display

        Args:
            answer: GeneratedAnswer object

        Returns:
            Formatted string for display
        """
        output = []

        # Header
        output.append("\n" + "=" * 80)
        output.append("ğŸ“‹ Káº¾T QUáº¢ TRáº¢ Lá»œI")
        output.append("=" * 80)

        # Question
        output.append(f"\nâ“ CÃ¢u há»i: {answer.question}")
        output.append(f"ğŸ¯ Intent: {answer.intent}")
        output.append(f"ğŸ“Š Äá»™ tin cáº­y: {answer.confidence:.0%}")

        # Natural language answer
        output.append("\n" + "-" * 80)
        output.append("ğŸ’¬ TRáº¢ Lá»œI:")
        output.append("-" * 80)
        output.append(answer.answer)

        # Structured data
        if answer.structured_data:
            output.append("\n" + "-" * 80)
            output.append("ğŸ“Š Dá»® LIá»†U CÃ“ Cáº¤U TRÃšC (JSON):")
            output.append("-" * 80)
            output.append(json.dumps(answer.structured_data, ensure_ascii=False, indent=2))

        # Sources
        output.append("\n" + "-" * 80)
        output.append(f"ğŸ“š NGUá»’N THAM KHáº¢O ({len(answer.sources)} nguá»“n):")
        output.append("-" * 80)

        for i, source in enumerate(answer.sources, 1):
            output.append(f"\n[{i}] {source.thu_tuc_name}")
            output.append(f"    MÃ£ thá»§ tá»¥c: {source.thu_tuc_code}")
            output.append(f"    Chunk ID: {source.chunk_id}")
            output.append(f"    Loáº¡i: {source.chunk_type}")
            output.append(f"    Äá»™ liÃªn quan: {source.relevance_score:.4f}")
            output.append(f"    Ná»™i dung: {source.content_snippet}")

        # Footer
        output.append("\n" + "=" * 80)
        output.append(f"â° Thá»i gian: {answer.timestamp}")
        output.append("=" * 80)

        return "\n".join(output)

    def export_answer_json(self, answer: GeneratedAnswer, filepath: str):
        """
        Export answer to JSON file

        Args:
            answer: GeneratedAnswer object
            filepath: Output file path
        """
        # Convert to dict
        answer_dict = asdict(answer)

        # Write to file
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(answer_dict, f, ensure_ascii=False, indent=2)

        print(f"âœ… Answer exported to: {filepath}")


def test_answer_generator():
    """Test answer generator with sample data"""
    print("=" * 80)
    print("TEST ANSWER GENERATOR")
    print("=" * 80)

    # Initialize generator
    generator = OllamaAnswerGenerator(model_name="qwen3:8b")

    # Mock retrieval result
    question = "ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬?"
    intent = "documents"
    confidence = 0.75

    context = """
================================================================================
[CHUNK 1] THá»¦ Tá»¤C: ÄÄƒng kÃ½ káº¿t hÃ´n
MÃ£: 1.013124
LÄ©nh vá»±c: Há»™ tá»‹ch
Chunk type: child_documents
================================================================================

[OVERVIEW]
Thá»§ tá»¥c Ä‘Äƒng kÃ½ káº¿t hÃ´n theo quy Ä‘á»‹nh cá»§a phÃ¡p luáº­t Viá»‡t Nam.

[DETAILED INFO]
Há»“ sÆ¡ bao gá»“m:
- Giáº¥y tá» tÃ¹y thÃ¢n (CMND/CCCD) cá»§a cáº£ hai bÃªn
- Giáº¥y xÃ¡c nháº­n tÃ¬nh tráº¡ng hÃ´n nhÃ¢n (náº¿u cáº§n)
- Giáº¥y khÃ¡m sá»©c khá»e tiá»n hÃ´n nhÃ¢n
- ÄÆ¡n Ä‘Äƒng kÃ½ káº¿t hÃ´n (theo máº«u)
Sá»‘ lÆ°á»£ng há»“ sÆ¡: 02 bá»™
"""

    retrieved_chunks = [
        {
            "chunk_id": "chunk_001_child_001",
            "thu_tuc_id": "1.013124",
            "chunk_type": "child_documents",
            "content": context,
            "final_score": 0.85,
            "metadata": {
                "tÃªn_thá»§_tá»¥c": "ÄÄƒng kÃ½ káº¿t hÃ´n",
                "mÃ£_thá»§_tá»¥c": "1.013124",
                "lÄ©nh_vá»±c": "Há»™ tá»‹ch"
            }
        }
    ]

    metadata = {
        "num_parent_chunks": 2,
        "num_child_chunks": 3,
        "query_variations": ["ÄÄƒng kÃ½ káº¿t hÃ´n cáº§n giáº¥y tá» gÃ¬?"]
    }

    # Generate answer
    answer = generator.generate(
        question=question,
        intent=intent,
        context=context,
        retrieved_chunks=retrieved_chunks,
        confidence=confidence,
        metadata=metadata
    )

    # Display answer
    print(generator.format_answer_for_display(answer))

    # Export to JSON
    generator.export_answer_json(answer, "test_answer.json")

    print("\n" + "=" * 80)
    print("TEST COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    test_answer_generator()
