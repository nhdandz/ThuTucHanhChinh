#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Chain-of-Verification (CoVe) - Layer 5 Validation
Multi-step verification process to reduce hallucinations

Steps:
1. Generate baseline answer
2. LLM plans verification questions
3. Answer verification questions independently
4. Generate final verified answer using verifications
"""

import sys
import json
import re
import requests
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass

if sys.platform == 'win32':
    sys.stdout.reconfigure(encoding='utf-8')


@dataclass
class VerificationQuestion:
    """A verification question for fact-checking"""
    question: str
    answer: str
    verified_fact: str
    confidence: float


@dataclass
class CoVeResult:
    """Result of Chain-of-Verification"""
    original_question: str
    baseline_answer: str
    verification_questions: List[VerificationQuestion]
    final_answer: str
    confidence_improvement: float
    is_verified: bool


class ChainOfVerification:
    """
    Chain-of-Verification (CoVe) Pipeline

    Process:
    1. Generate baseline answer
    2. LLM generates verification questions about the answer
    3. Answer each verification question independently
    4. Use verifications to generate final corrected answer
    """

    def __init__(
        self,
        model_name: str = "qwen3:8b",
        ollama_url: str = "http://localhost:11434",
        num_verifications: int = 5
    ):
        """
        Initialize CoVe pipeline

        Args:
            model_name: Ollama model name
            ollama_url: Ollama server URL
            num_verifications: Number of verification questions to generate
        """
        print(f"üîÑ Initializing Chain-of-Verification (CoVe)")
        print(f"   Model: {model_name}")
        print(f"   Verification questions: {num_verifications}")

        self.model_name = model_name
        self.ollama_url = ollama_url
        self.generate_endpoint = f"{ollama_url}/api/generate"
        self.num_verifications = num_verifications

        print(f"‚úÖ CoVe initialized!")

    def _call_ollama(
        self,
        prompt: str,
        system: Optional[str] = None,
        temperature: float = 0.2
    ) -> str:
        """Call Ollama API"""
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": temperature
            }
        }

        if system:
            payload["system"] = system

        try:
            response = requests.post(
                self.generate_endpoint,
                json=payload,
                timeout=120
            )

            if response.status_code != 200:
                raise Exception(f"Ollama API error: {response.status_code}")

            return response.json()["response"].strip()
        except Exception as e:
            print(f"‚ö†Ô∏è Ollama API call failed: {e}")
            return ""

    def _generate_baseline_answer(
        self,
        question: str,
        context: str
    ) -> str:
        """
        Generate initial baseline answer

        Args:
            question: User question
            context: Retrieved context

        Returns:
            Baseline answer
        """
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ th·ªß t·ª•c h√†nh ch√≠nh Vi·ªát Nam.
CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT ƒë∆∞·ª£c cung c·∫•p.
"""

        prompt = f"""C√¢u h·ªèi: {question}

Context:
{context[:2000]}

Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c:"""

        answer = self._call_ollama(prompt, system=system_prompt, temperature=0.1)
        return answer

    def _generate_verification_questions(
        self,
        question: str,
        answer: str
    ) -> List[str]:
        """
        Generate verification questions to fact-check the answer

        Args:
            question: Original question
            answer: Baseline answer to verify

        Returns:
            List of verification questions
        """
        system_prompt = """B·∫°n l√† chuy√™n gia fact-checking.

NHI·ªÜM V·ª§: T·∫°o c√°c c√¢u h·ªèi ki·ªÉm ch·ª©ng ƒë·ªÉ x√°c minh t√≠nh ch√≠nh x√°c c·ªßa c√¢u tr·∫£ l·ªùi.

Y√äU C·∫¶U:
- M·ªói c√¢u h·ªèi ki·ªÉm tra m·ªôt fact c·ª• th·ªÉ
- C√¢u h·ªèi ph·∫£i c√≥ th·ªÉ tr·∫£ l·ªùi YES/NO ho·∫∑c v·ªõi s·ªë c·ª• th·ªÉ
- T·∫≠p trung v√†o c√°c con s·ªë, danh s√°ch, th·ªùi gian
"""

        prompt = f"""C√¢u h·ªèi g·ªëc: "{question}"

C√¢u tr·∫£ l·ªùi c·∫ßn ki·ªÉm ch·ª©ng:
{answer}

H√£y t·∫°o {self.num_verifications} c√¢u h·ªèi ki·ªÉm ch·ª©ng ƒë·ªÉ x√°c minh c√°c fact trong c√¢u tr·∫£ l·ªùi.

V√ç D·ª§:
N·∫øu c√¢u tr·∫£ l·ªùi n√≥i "c·∫ßn 3 gi·∫•y t·ªù", t·∫°o c√¢u h·ªèi: "C√≥ ch√≠nh x√°c 3 lo·∫°i gi·∫•y t·ªù kh√¥ng?"
N·∫øu n√≥i "trong 7 ng√†y", t·∫°o c√¢u h·ªèi: "Th·ªùi h·∫°n c√≥ ƒë√∫ng l√† 7 ng√†y kh√¥ng?"

Tr·∫£ v·ªÅ JSON array:
["C√¢u h·ªèi ki·ªÉm ch·ª©ng 1?", "C√¢u h·ªèi ki·ªÉm ch·ª©ng 2?", ...]

Ch·ªâ tr·∫£ v·ªÅ JSON array, kh√¥ng gi·∫£i th√≠ch:"""

        response = self._call_ollama(prompt, system=system_prompt, temperature=0.3)

        if not response:
            return []

        try:
            # Extract JSON
            start = response.find("[")
            end = response.rfind("]") + 1

            if start != -1 and end > start:
                json_str = response[start:end]
                questions = json.loads(json_str)
                return questions[:self.num_verifications]
        except:
            pass

        return []

    def _answer_verification_question(
        self,
        verification_q: str,
        context: str
    ) -> Tuple[str, float]:
        """
        Answer a verification question using context

        Args:
            verification_q: Verification question
            context: Source context

        Returns:
            Tuple of (answer, confidence)
        """
        system_prompt = """B·∫°n l√† h·ªá th·ªëng fact-checking.
CH·ªà tr·∫£ l·ªùi d·ª±a tr√™n CONTEXT.
Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c.
"""

        prompt = f"""Context:
{context[:1500]}

C√¢u h·ªèi ki·ªÉm ch·ª©ng: {verification_q}

Tr·∫£ l·ªùi d·ª±a HO√ÄN TO√ÄN v√†o context:
Answer:
Confidence: [0.0-1.0]

Ch·ªâ tr·∫£ v·ªÅ format tr√™n:"""

        response = self._call_ollama(prompt, system=system_prompt, temperature=0.1)

        if not response:
            return "Kh√¥ng c√≥ th√¥ng tin", 0.0

        # Parse response
        lines = response.split('\n')
        answer = "Kh√¥ng c√≥ th√¥ng tin"
        confidence = 0.5

        for line in lines:
            if line.startswith('Answer:'):
                answer = line.split('Answer:', 1)[1].strip()
            elif line.startswith('Confidence:'):
                try:
                    conf_str = re.search(r'(\d+\.?\d*)', line)
                    if conf_str:
                        confidence = float(conf_str.group(1))
                        if confidence > 1.0:
                            confidence /= 100.0
                except:
                    pass

        return answer, confidence

    def _generate_verified_answer(
        self,
        question: str,
        baseline_answer: str,
        verifications: List[VerificationQuestion],
        context: str
    ) -> str:
        """
        Generate final verified answer using verification results

        Args:
            question: Original question
            baseline_answer: Initial answer
            verifications: Verification results
            context: Source context

        Returns:
            Final verified answer
        """
        system_prompt = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n v·ªÅ th·ªß t·ª•c h√†nh ch√≠nh Vi·ªát Nam.

NHI·ªÜM V·ª§: T·∫°o c√¢u tr·∫£ l·ªùi CH√çNH X√ÅC d·ª±a tr√™n:
1. Context g·ªëc
2. K·∫øt qu·∫£ ki·ªÉm ch·ª©ng c√°c fact

CH·ªà s·ª≠ d·ª•ng c√°c fact ƒë√£ ƒë∆∞·ª£c ki·ªÉm ch·ª©ng.
KH√îNG th√™m th√¥ng tin kh√¥ng c√≥ trong context ho·∫∑c verifications.
"""

        # Format verifications
        verification_text = "\n".join([
            f"Q: {v.question}\nA: {v.answer} (confidence: {v.confidence:.2f})"
            for v in verifications
        ])

        prompt = f"""C√¢u h·ªèi g·ªëc: {question}

C√¢u tr·∫£ l·ªùi ban ƒë·∫ßu:
{baseline_answer}

K·∫øt qu·∫£ ki·ªÉm ch·ª©ng:
{verification_text}

Context g·ªëc:
{context[:1500]}

D·ª±a tr√™n k·∫øt qu·∫£ ki·ªÉm ch·ª©ng v√† context, h√£y t·∫°o c√¢u tr·∫£ l·ªùi CH√çNH X√ÅC nh·∫•t.
Ch·ªâ s·ª≠ d·ª•ng th√¥ng tin ƒë√£ ƒë∆∞·ª£c x√°c minh.

C√¢u tr·∫£ l·ªùi cu·ªëi c√πng:"""

        final_answer = self._call_ollama(prompt, system=system_prompt, temperature=0.1)
        return final_answer

    def verify_answer(
        self,
        question: str,
        context: str
    ) -> CoVeResult:
        """
        Run complete Chain-of-Verification pipeline

        Args:
            question: User question
            context: Retrieved context

        Returns:
            CoVeResult object
        """
        print(f"\nüîç Chain-of-Verification (CoVe) Pipeline...")
        print(f"   Question: {question}")

        # Step 1: Generate baseline answer
        print(f"\n   [1/4] Generating baseline answer...")
        baseline_answer = self._generate_baseline_answer(question, context)
        print(f"         ‚úì Baseline generated ({len(baseline_answer)} chars)")

        # Step 2: Generate verification questions
        print(f"\n   [2/4] Planning verification questions...")
        verification_questions = self._generate_verification_questions(question, baseline_answer)
        print(f"         ‚úì {len(verification_questions)} questions generated")

        # Step 3: Answer verification questions
        print(f"\n   [3/4] Answering verification questions...")
        verifications = []

        for i, ver_q in enumerate(verification_questions, 1):
            print(f"         [{i}/{len(verification_questions)}] {ver_q[:60]}...")

            answer, confidence = self._answer_verification_question(ver_q, context)

            verification = VerificationQuestion(
                question=ver_q,
                answer=answer,
                verified_fact=f"{ver_q} ‚Üí {answer}",
                confidence=confidence
            )

            verifications.append(verification)
            print(f"             Answer: {answer[:50]}... (conf: {confidence:.2f})")

        # Step 4: Generate final verified answer
        print(f"\n   [4/4] Generating final verified answer...")
        final_answer = self._generate_verified_answer(
            question, baseline_answer, verifications, context
        )
        print(f"         ‚úì Final answer generated ({len(final_answer)} chars)")

        # Calculate confidence improvement
        baseline_length = len(baseline_answer.split())
        final_length = len(final_answer.split())
        avg_verification_conf = (
            sum(v.confidence for v in verifications) / len(verifications)
            if verifications else 0.0
        )

        # Simple heuristic: improvement based on verification confidence
        confidence_improvement = avg_verification_conf

        is_verified = avg_verification_conf >= 0.7

        result = CoVeResult(
            original_question=question,
            baseline_answer=baseline_answer,
            verification_questions=verifications,
            final_answer=final_answer,
            confidence_improvement=confidence_improvement,
            is_verified=is_verified
        )

        print(f"\n   üìä CoVe Summary:")
        print(f"      Verification questions: {len(verifications)}")
        print(f"      Avg verification confidence: {avg_verification_conf:.2f}")
        print(f"      Is verified: {'‚úÖ' if is_verified else '‚ùå'}")

        return result

    def format_cove_report(self, result: CoVeResult) -> str:
        """Format CoVe result as report"""
        lines = []

        lines.append("\n" + "=" * 80)
        lines.append("üìã CHAIN-OF-VERIFICATION REPORT")
        lines.append("=" * 80)

        lines.append(f"\n‚ùì Question: {result.original_question}")
        lines.append(f"\n‚úÖ Verified: {'YES' if result.is_verified else 'NO'}")
        lines.append(f"üìà Confidence: {result.confidence_improvement:.0%}")

        lines.append("\n" + "-" * 80)
        lines.append("BASELINE ANSWER:")
        lines.append("-" * 80)
        lines.append(result.baseline_answer)

        lines.append("\n" + "-" * 80)
        lines.append(f"VERIFICATION QUESTIONS ({len(result.verification_questions)}):")
        lines.append("-" * 80)

        for i, ver in enumerate(result.verification_questions, 1):
            lines.append(f"\n[{i}] {ver.question}")
            lines.append(f"    Answer: {ver.answer}")
            lines.append(f"    Confidence: {ver.confidence:.2f}")

        lines.append("\n" + "-" * 80)
        lines.append("FINAL VERIFIED ANSWER:")
        lines.append("-" * 80)
        lines.append(result.final_answer)

        lines.append("\n" + "=" * 80)

        return "\n".join(lines)


def test_chain_of_verification():
    """Test CoVe pipeline"""
    print("=" * 80)
    print("TEST CHAIN-OF-VERIFICATION")
    print("=" * 80)

    cove = ChainOfVerification(
        model_name="qwen3:8b",
        num_verifications=3  # Use 3 for faster testing
    )

    question = "ƒêƒÉng k√Ω k·∫øt h√¥n c·∫ßn gi·∫•y t·ªù g√¨?"
    context = """
H·ªì s∆° ƒëƒÉng k√Ω k·∫øt h√¥n bao g·ªìm:
1. Gi·∫•y t·ªù t√πy th√¢n (CMND/CCCD/H·ªô chi·∫øu) c·ªßa c·∫£ hai b√™n - 02 b·∫£n sao
2. Gi·∫•y x√°c nh·∫≠n t√¨nh tr·∫°ng h√¥n nh√¢n (ƒë·ªëi v·ªõi ng∆∞·ªùi t·ª´ 30 tu·ªïi tr·ªü l√™n) - 01 b·∫£n ch√≠nh
3. Gi·∫•y kh√°m s·ª©c kh·ªèe ti·ªÅn h√¥n nh√¢n do c∆° s·ªü y t·∫ø c·∫•p - 01 b·∫£n ch√≠nh

Th·ªùi gian gi·∫£i quy·∫øt: Trong ng√†y l√†m vi·ªác, tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát kh√¥ng qu√° 03 ng√†y.
S·ªë l∆∞·ª£ng h·ªì s∆°: 02 b·ªô
"""

    result = cove.verify_answer(question, context)
    print(cove.format_cove_report(result))

    print("\n" + "=" * 80)
    print("TEST COMPLETE")
    print("=" * 80)


if __name__ == "__main__":
    test_chain_of_verification()
